{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067deb56",
   "metadata": {},
   "source": [
    "# Pre1 + in2\n",
    "1. Reweighting + Adversarial Debiasing\n",
    "2. Disparate Impact Remover + Adversarial Debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c59c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "source": [
    "##### PACKAGES\n",
    "\n",
    "# working paths\n",
    "%run code_00_working_paths.py\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.preprocessing.reweighing import Reweighing\n",
    "from aif360.algorithms.preprocessing.lfr import LFR\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(func_path)\n",
    "\n",
    "from load_data import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfe9838",
   "metadata": {},
   "source": [
    "## Parameters and preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "074c549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PARAMETERS\n",
    "\n",
    "# specify data set\n",
    "# one of ['data1', 'data2', ..., 'data50']\n",
    "data = 'data1'\n",
    "\n",
    "# partitioning\n",
    "num_folds = 10\n",
    "seed      = 1\n",
    "use_fold  = 0 # one of [0, 1, ..., 4 (num_folds-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f7d1509",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PRE-PROCESSOR PARAMS\n",
    "\n",
    "all_lambda = [0.5,0.6,0.7,0.8,0.9,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e9cf975",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### IN-PROCESSOR PARAMS\n",
    "\n",
    "adversary_loss_weight = 0.1 # other options: [0.1, 0.01, 0.001]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7713a439",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b55fb0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RANDOM SEED\n",
    "\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59641a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 6)\n"
     ]
    }
   ],
   "source": [
    "##### LOAD PARTITIONING\n",
    "\n",
    "# Assuming data is in the format 'dataN' where N is the dataset number\n",
    "dataset_number = data[4:]\n",
    "\n",
    "# Create the directory path (assuming directories data1, data2, ..., data50 already exist)\n",
    "input_dir = os.path.join(data_path, 'prepared')\n",
    "\n",
    "# Construct the full file path\n",
    "file_path = os.path.join(input_dir, data + '_orig_test.pkl')\n",
    "\n",
    "# Load the dataset\n",
    "with open(file_path, 'rb') as file:\n",
    "    dataset_orig_test = pickle.load(file)\n",
    "    \n",
    "# Convert to dataframe and print the shape\n",
    "te = dataset_orig_test.convert_to_dataframe()[0]\n",
    "print(te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "893921bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DATA PREP\n",
    "\n",
    "# protected attribute\n",
    "protected           = 'race'\n",
    "privileged_groups   = [{'race': 1}] \n",
    "unprivileged_groups = [{'race': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf10621",
   "metadata": {},
   "source": [
    "## Fair processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e655ce70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_2760\\2778220032.py:4: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_2760\\2778220032.py:4: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_2760\\2778220032.py:7: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_2760\\2778220032.py:7: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "- METHOD: RW...\n",
      "------------------------------\n",
      "------------------------------\n",
      "- FOLD 0...\n",
      "------------------------------\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:164: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:164: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.695921; batch adversarial loss: 0.881330\n",
      "epoch 1; iter: 0; batch classifier loss: 0.684307; batch adversarial loss: 0.902722\n",
      "epoch 2; iter: 0; batch classifier loss: 0.715801; batch adversarial loss: 0.914391\n",
      "epoch 3; iter: 0; batch classifier loss: 0.720084; batch adversarial loss: 0.919068\n",
      "epoch 4; iter: 0; batch classifier loss: 0.735882; batch adversarial loss: 0.929588\n",
      "epoch 5; iter: 0; batch classifier loss: 0.725645; batch adversarial loss: 0.917140\n",
      "epoch 6; iter: 0; batch classifier loss: 0.771810; batch adversarial loss: 0.941367\n",
      "epoch 7; iter: 0; batch classifier loss: 0.784690; batch adversarial loss: 0.953206\n",
      "epoch 8; iter: 0; batch classifier loss: 0.811011; batch adversarial loss: 0.964823\n",
      "epoch 9; iter: 0; batch classifier loss: 0.833914; batch adversarial loss: 0.937362\n",
      "epoch 10; iter: 0; batch classifier loss: 0.839917; batch adversarial loss: 0.941357\n",
      "epoch 11; iter: 0; batch classifier loss: 0.850234; batch adversarial loss: 0.937876\n",
      "epoch 12; iter: 0; batch classifier loss: 0.863745; batch adversarial loss: 0.962226\n",
      "epoch 13; iter: 0; batch classifier loss: 0.932996; batch adversarial loss: 0.974950\n",
      "epoch 14; iter: 0; batch classifier loss: 0.947917; batch adversarial loss: 0.948702\n",
      "epoch 15; iter: 0; batch classifier loss: 0.917237; batch adversarial loss: 0.960919\n",
      "epoch 16; iter: 0; batch classifier loss: 0.970396; batch adversarial loss: 0.977572\n",
      "epoch 17; iter: 0; batch classifier loss: 0.982946; batch adversarial loss: 0.957260\n",
      "epoch 18; iter: 0; batch classifier loss: 1.136505; batch adversarial loss: 0.969777\n",
      "epoch 19; iter: 0; batch classifier loss: 1.037020; batch adversarial loss: 0.952490\n",
      "epoch 20; iter: 0; batch classifier loss: 1.011019; batch adversarial loss: 0.948988\n",
      "epoch 21; iter: 0; batch classifier loss: 1.025409; batch adversarial loss: 0.947345\n",
      "epoch 22; iter: 0; batch classifier loss: 1.108943; batch adversarial loss: 0.951285\n",
      "epoch 23; iter: 0; batch classifier loss: 1.119310; batch adversarial loss: 0.937847\n",
      "epoch 24; iter: 0; batch classifier loss: 1.128169; batch adversarial loss: 0.931473\n",
      "epoch 25; iter: 0; batch classifier loss: 1.140989; batch adversarial loss: 0.940935\n",
      "epoch 26; iter: 0; batch classifier loss: 1.143216; batch adversarial loss: 0.926246\n",
      "epoch 27; iter: 0; batch classifier loss: 1.191921; batch adversarial loss: 0.926472\n",
      "epoch 28; iter: 0; batch classifier loss: 1.176827; batch adversarial loss: 0.936057\n",
      "epoch 29; iter: 0; batch classifier loss: 1.154373; batch adversarial loss: 0.923909\n",
      "epoch 30; iter: 0; batch classifier loss: 1.202171; batch adversarial loss: 0.910000\n",
      "epoch 31; iter: 0; batch classifier loss: 1.246139; batch adversarial loss: 0.929934\n",
      "epoch 32; iter: 0; batch classifier loss: 1.372920; batch adversarial loss: 0.909644\n",
      "epoch 33; iter: 0; batch classifier loss: 1.246366; batch adversarial loss: 0.902598\n",
      "epoch 34; iter: 0; batch classifier loss: 1.224923; batch adversarial loss: 0.895255\n",
      "epoch 35; iter: 0; batch classifier loss: 1.218082; batch adversarial loss: 0.884351\n",
      "epoch 36; iter: 0; batch classifier loss: 1.160296; batch adversarial loss: 0.877149\n",
      "epoch 37; iter: 0; batch classifier loss: 1.295199; batch adversarial loss: 0.880098\n",
      "epoch 38; iter: 0; batch classifier loss: 1.178379; batch adversarial loss: 0.872258\n",
      "epoch 39; iter: 0; batch classifier loss: 1.348203; batch adversarial loss: 0.873254\n",
      "epoch 40; iter: 0; batch classifier loss: 1.389910; batch adversarial loss: 0.880262\n",
      "epoch 41; iter: 0; batch classifier loss: 1.191896; batch adversarial loss: 0.869908\n",
      "epoch 42; iter: 0; batch classifier loss: 1.285012; batch adversarial loss: 0.867308\n",
      "epoch 43; iter: 0; batch classifier loss: 1.324812; batch adversarial loss: 0.862794\n",
      "epoch 44; iter: 0; batch classifier loss: 1.254659; batch adversarial loss: 0.855560\n",
      "epoch 45; iter: 0; batch classifier loss: 1.189628; batch adversarial loss: 0.840852\n",
      "epoch 46; iter: 0; batch classifier loss: 1.239707; batch adversarial loss: 0.848658\n",
      "epoch 47; iter: 0; batch classifier loss: 1.285361; batch adversarial loss: 0.839208\n",
      "epoch 48; iter: 0; batch classifier loss: 1.153560; batch adversarial loss: 0.832742\n",
      "epoch 49; iter: 0; batch classifier loss: 1.301169; batch adversarial loss: 0.833788\n",
      "\n",
      "------------------------------\n",
      "- FOLD 1...\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 74\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mvariable_scope(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdebiased_classifier\u001b[39m\u001b[38;5;124m'\u001b[39m, reuse\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mAUTO_REUSE):\n\u001b[0;32m     68\u001b[0m     debiased_model \u001b[38;5;241m=\u001b[39m AdversarialDebiasing(privileged_groups\u001b[38;5;241m=\u001b[39mprivileged_groups,\n\u001b[0;32m     69\u001b[0m                                           unprivileged_groups\u001b[38;5;241m=\u001b[39munprivileged_groups,\n\u001b[0;32m     70\u001b[0m                                           debias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     71\u001b[0m                                           adversary_loss_weight\u001b[38;5;241m=\u001b[39madversary_loss_weight,\n\u001b[0;32m     72\u001b[0m                                           scope_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdebiased_classifier\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     73\u001b[0m                                           sess\u001b[38;5;241m=\u001b[39msess)\n\u001b[1;32m---> 74\u001b[0m     debiased_model\u001b[38;5;241m.\u001b[39mfit(dataset_transf_train)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# apply the model to valid data\u001b[39;00m\n\u001b[0;32m     77\u001b[0m scores_valid \u001b[38;5;241m=\u001b[39m debiased_model\u001b[38;5;241m.\u001b[39mpredict(dataset_transf_valid)\u001b[38;5;241m.\u001b[39mscores\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\transformer.py:27\u001b[0m, in \u001b[0;36maddmetadata.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 27\u001b[0m     new_dataset \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_dataset, Dataset):\n\u001b[0;32m     29\u001b[0m         new_dataset\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m new_dataset\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:181\u001b[0m, in \u001b[0;36mAdversarialDebiasing.fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (grad,var) \u001b[38;5;129;01min\u001b[39;00m classifier_opt\u001b[38;5;241m.\u001b[39mcompute_gradients(pred_labels_loss, var_list\u001b[38;5;241m=\u001b[39mclassifier_vars):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebias:\n\u001b[1;32m--> 181\u001b[0m         unit_adversary_grad \u001b[38;5;241m=\u001b[39m normalize(adversary_grads[var])\n\u001b[0;32m    182\u001b[0m         grad \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(grad \u001b[38;5;241m*\u001b[39m unit_adversary_grad) \u001b[38;5;241m*\u001b[39m unit_adversary_grad\n\u001b[0;32m    183\u001b[0m         grad \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madversary_loss_weight \u001b[38;5;241m*\u001b[39m adversary_grads[var]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:176\u001b[0m, in \u001b[0;36mAdversarialDebiasing.fit.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Update classifier parameters\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     adversary_grads \u001b[38;5;241m=\u001b[39m {var: grad \u001b[38;5;28;01mfor\u001b[39;00m (grad, var) \u001b[38;5;129;01min\u001b[39;00m adversary_opt\u001b[38;5;241m.\u001b[39mcompute_gradients(pred_protected_attributes_loss,\n\u001b[0;32m    175\u001b[0m                                                                           var_list\u001b[38;5;241m=\u001b[39mclassifier_vars)}\n\u001b[1;32m--> 176\u001b[0m normalize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;241m/\u001b[39m (tf\u001b[38;5;241m.\u001b[39mnorm(x) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mfinfo(np\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mtiny)\n\u001b[0;32m    178\u001b[0m classifier_grads \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (grad,var) \u001b[38;5;129;01min\u001b[39;00m classifier_opt\u001b[38;5;241m.\u001b[39mcompute_gradients(pred_labels_loss, var_list\u001b[38;5;241m=\u001b[39mclassifier_vars):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py:509\u001b[0m, in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone values not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    510\u001b[0m   \u001b[38;5;66;03m# if dtype is provided, forces numpy array to be the type\u001b[39;00m\n\u001b[0;32m    511\u001b[0m   \u001b[38;5;66;03m# provided if possible.\u001b[39;00m\n\u001b[0;32m    512\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mand\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mis_numpy_compatible:\n",
      "\u001b[1;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": [
    "##### MODELING: RW and AD\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Disable TensorFlow eager execution\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Create a TensorFlow session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Define the normalization function\n",
    "def safe_normalize(x):\n",
    "    norm = tf.norm(x)\n",
    "    print(\"x:\", x)\n",
    "    print(\"norm:\", norm)\n",
    "    return tf.cond(tf.not_equal(norm, 0), lambda: x / norm, lambda: x)\n",
    "\n",
    "# timer\n",
    "cv_start = time.time()\n",
    "\n",
    "# Create the directory path for inprocessor output\n",
    "output_dir = os.path.join(res_path, 'pre1in2')\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# preprocessing and inprocessing loop\n",
    "print('-' * 30)\n",
    "print('- METHOD: RW...')\n",
    "print('-' * 30)\n",
    "\n",
    "# loop through fold combinations\n",
    "for fold in range(num_folds):\n",
    "    \n",
    "    # feedback\n",
    "    print('-'*30)\n",
    "    print('- FOLD ' + str(fold) + '...')\n",
    "    print('-'*30)\n",
    "\n",
    "    ##### LOAD DATA\n",
    "    # import data subsets\n",
    "    train_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_train.pkl')\n",
    "    valid_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_valid.pkl')\n",
    "    test_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_test.pkl')\n",
    "\n",
    "    # Load the dataset\n",
    "    with open(train_path, 'rb') as file:\n",
    "        data_train = pickle.load(file)\n",
    "    with open(valid_path, 'rb') as file:\n",
    "        data_valid = pickle.load(file)\n",
    "    with open(test_path, 'rb') as file:\n",
    "        data_test = pickle.load(file)\n",
    "\n",
    "    # Reweighing\n",
    "    RW = Reweighing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "    RW.fit(data_train)\n",
    "\n",
    "    # Transform the data\n",
    "    dataset_transf_train = RW.transform(data_train)\n",
    "    dataset_transf_valid = RW.transform(data_valid)\n",
    "    dataset_transf_test = RW.transform(data_test)\n",
    "\n",
    "    ##### IN-PROCESSOR: ADVERSARIAL DEBIASING\n",
    "\n",
    "    # fit adversarial debiasing\n",
    "    with tf.variable_scope('debiased_classifier', reuse=tf.AUTO_REUSE):\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups=privileged_groups,\n",
    "                                              unprivileged_groups=unprivileged_groups,\n",
    "                                              debias=True,\n",
    "                                              adversary_loss_weight=adversary_loss_weight,\n",
    "                                              scope_name='debiased_classifier',\n",
    "                                              sess=sess)\n",
    "        debiased_model.fit(dataset_transf_train)\n",
    "\n",
    "    # apply the model to valid data\n",
    "    scores_valid = debiased_model.predict(dataset_transf_valid).scores\n",
    "    scores_valid_flat = scores_valid.flatten()\n",
    "    advdebias_predictions = pd.DataFrame()\n",
    "    advdebias_predictions['scores'] = scores_valid_flat\n",
    "    advdebias_predictions['targets'] = dataset_transf_valid.labels.flatten()\n",
    "    advdebias_predictions.to_csv(os.path.join(output_dir, data + '_' + str(fold) + '_AD_RW_predictions_valid.csv'), \n",
    "                                 index=None, \n",
    "                                 header=True)\n",
    "\n",
    "    # apply the model to test data\n",
    "    scores_test = debiased_model.predict(dataset_transf_test).scores\n",
    "    scores_test_flat = scores_test.flatten()\n",
    "\n",
    "    advdebias_predictions = pd.DataFrame()\n",
    "    advdebias_predictions['scores'] = scores_test_flat\n",
    "    advdebias_predictions.to_csv(os.path.join(output_dir, data + '_' + str(fold) + '_AD_RW_predictions_test.csv'), \n",
    "                                 index=None, \n",
    "                                 header=True)\n",
    "\n",
    "    print('')\n",
    "\n",
    "##### END LOOP\n",
    "\n",
    "# feedback\n",
    "print('')\n",
    "\n",
    "# print performance\n",
    "print('')\n",
    "print('Finished in {:.2f} minutes'.format((time.time() - cv_start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b09ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MODELING: DI and AD\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Disable TensorFlow eager execution\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Create a TensorFlow session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Define the normalization function\n",
    "def safe_normalize(x):\n",
    "    norm = tf.norm(x)\n",
    "    print(\"x:\", x)\n",
    "    print(\"norm:\", norm)\n",
    "    return tf.cond(tf.not_equal(norm, 0), lambda: x / norm, lambda: x)\n",
    "\n",
    "##### MODELING\n",
    "\n",
    "# timer\n",
    "cv_start = time.time()\n",
    "\n",
    "# Create the directory path for inprocessor output\n",
    "output_dir = os.path.join(res_path, 'pre1in2')\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# preprocessing and inprocessing loop\n",
    "print('-' * 30)\n",
    "print('- METHOD: DI...')\n",
    "print('-' * 30)\n",
    "\n",
    "# loop through fold combinations\n",
    "for fold in range(num_folds):\n",
    "    \n",
    "    # feedback\n",
    "    print('-'*30)\n",
    "    print('- FOLD ' + str(fold) + '...')\n",
    "    print('-'*30)\n",
    "\n",
    "    ##### LOAD DATA\n",
    "    # import data subsets\n",
    "    train_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_train.pkl')\n",
    "    valid_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_valid.pkl')\n",
    "    test_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_test.pkl')\n",
    "\n",
    "    # Load the dataset\n",
    "    with open(train_path, 'rb') as file:\n",
    "        data_train = pickle.load(file)\n",
    "    with open(valid_path, 'rb') as file:\n",
    "        data_valid = pickle.load(file)\n",
    "    with open(test_path, 'rb') as file:\n",
    "        data_test = pickle.load(file)\n",
    "\n",
    "    for i in all_lambda:\n",
    "        # Disparate Impact Remover\n",
    "        di = DisparateImpactRemover(repair_level=i, sensitive_attribute=protected)\n",
    "        \n",
    "        # Transform the data\n",
    "        dataset_transf_train = di.fit_transform(data_train)\n",
    "        dataset_transf_valid = di.fit_transform(data_valid)\n",
    "        dataset_transf_test = di.fit_transform(data_test)\n",
    "        \n",
    "        ##### IN-PROCESSOR: ADVERSARIAL DEBIASING\n",
    "        \n",
    "        # fit adversarial debiasing\n",
    "        with tf.variable_scope('debiased_classifier', reuse=tf.AUTO_REUSE):\n",
    "            debiased_model = AdversarialDebiasing(privileged_groups=privileged_groups,\n",
    "                                                  unprivileged_groups=unprivileged_groups,\n",
    "                                                  debias=True,\n",
    "                                                  adversary_loss_weight=adversary_loss_weight,\n",
    "                                                  scope_name='debiased_classifier',\n",
    "                                                  sess=sess)\n",
    "            debiased_model.fit(dataset_transf_train)\n",
    "\n",
    "        # apply the model to valid data\n",
    "        scores_valid = debiased_model.predict(dataset_transf_valid).scores\n",
    "        scores_valid_flat = scores_valid.flatten()\n",
    "        advdebias_predictions = pd.DataFrame()\n",
    "        advdebias_predictions['scores'] = scores_valid_flat\n",
    "        advdebias_predictions['targets'] = dataset_transf_valid.labels.flatten()\n",
    "        advdebias_predictions.to_csv(os.path.join(output_dir, data + '_' + str(fold) + '_AD_DI_' + str(i) + '_predictions_valid.csv'), \n",
    "                                     index=None, \n",
    "                                     header=True)\n",
    "\n",
    "        # apply the model to test data\n",
    "        scores_test = debiased_model.predict(dataset_transf_test).scores\n",
    "        scores_test_flat = scores_test.flatten()\n",
    "\n",
    "        advdebias_predictions = pd.DataFrame()\n",
    "        advdebias_predictions['scores'] = scores_test_flat\n",
    "        advdebias_predictions.to_csv(os.path.join(output_dir, data + '_' + str(fold) + '_AD_DI_' + str(i) + '_predictions_test.csv'), \n",
    "                                     index=None, \n",
    "                                     header=True)\n",
    "        print('')\n",
    "\n",
    "##### END LOOP\n",
    "\n",
    "# feedback\n",
    "print('')\n",
    "\n",
    "# print performance\n",
    "print('')\n",
    "print('Finished in {:.2f} minutes'.format((time.time() - cv_start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768a9ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
