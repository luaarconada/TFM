{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAIR PRE-PROCESSING\n",
    "\n",
    "This notebook implements the following pre-processors:\n",
    "- Reweighting [(Calders et al. 2009)](https://ieeexplore.ieee.org/abstract/document/5360534)\n",
    "- Disparate Impact Remover [(Feldman et al. 2015)](https://dl.acm.org/doi/abs/10.1145/2783258.2783311?casa_token=hPPsvh9w2QEAAAAA:RE90pNifv99Y9yCMgE4O1vOquljiAtjVCQQ3UgFDHIgcn2J21J5ry6HCv2iXXTX2Gw9e1VBbS07j)\n",
    "\n",
    "A further analysis of the processor outputs is performed in `code_02_preprocess2.R` and `code_03_preprocess3.R`.\n",
    "\n",
    "The notebook loads the data exported in `code_00_partitinoing.ipynb` and applies pre-processors. The transformed training and test data is exported as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PACKAGES\n",
    "\n",
    "# working paths\n",
    "%run code_00_working_paths.py\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.preprocessing.reweighing import Reweighing\n",
    "from aif360.algorithms.preprocessing.lfr import LFR\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(func_path)\n",
    "\n",
    "from load_data import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameters and preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### PARAMETERS\n",
    "\n",
    "# specify data set\n",
    "# one of ['data1', 'data2', ..., 'data50']\n",
    "data = 'datacorr5'\n",
    "\n",
    "# partitioning\n",
    "num_folds = 10\n",
    "seed      = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PRE-PROCESSOR PARAMS\n",
    "\n",
    "all_lambda = [0.5,0.6,0.7,0.8,0.9,1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RANDOM SEED\n",
    "\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 6)\n"
     ]
    }
   ],
   "source": [
    "##### LOAD PARTITIONING\n",
    "\n",
    "# Assuming data is in the format 'dataN' where N is the dataset number\n",
    "dataset_number = data[4:]\n",
    "\n",
    "# Create the directory path (assuming directories data1, data2, ..., data50 already exist)\n",
    "input_dir = os.path.join(data_path, 'prepared')\n",
    "\n",
    "# Construct the full file path\n",
    "file_path = os.path.join(input_dir, data + '_orig_test.pkl')\n",
    "\n",
    "# Load the dataset\n",
    "with open(file_path, 'rb') as file:\n",
    "    dataset_orig_test = pickle.load(file)\n",
    "    \n",
    "# Convert to dataframe and print the shape\n",
    "te = dataset_orig_test.convert_to_dataframe()[0]\n",
    "print(te.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### DATA PREP\n",
    "\n",
    "# protected attribute\n",
    "protected           = 'race'\n",
    "privileged_groups   = [{'race': 1}] \n",
    "unprivileged_groups = [{'race': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fair processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "- METHOD: RW...\n",
      "------------------------------\n",
      "-- achieved a statistical parity difference between unprivileged and privileged groups = -0.000000\n",
      "-- achieved a statistical parity difference between unprivileged and privileged groups = -0.000000\n",
      "-- achieved a statistical parity difference between unprivileged and privileged groups = 0.000000\n",
      "-- achieved a statistical parity difference between unprivileged and privileged groups = -0.000000\n",
      "-- achieved a statistical parity difference between unprivileged and privileged groups = 0.000000\n",
      "-- achieved a statistical parity difference between unprivileged and privileged groups = -0.000000\n",
      "-- achieved a statistical parity difference between unprivileged and privileged groups = -0.000000\n",
      "-- achieved a statistical parity difference between unprivileged and privileged groups = 0.000000\n",
      "-- achieved a statistical parity difference between unprivileged and privileged groups = 0.000000\n",
      "-- achieved a statistical parity difference between unprivileged and privileged groups = -0.000000\n",
      "\n",
      "------------------------------\n",
      "- METHOD: DI...\n",
      "------------------------------\n",
      "-- FOLD 0...\n",
      "-- FOLD 1...\n",
      "-- FOLD 2...\n",
      "-- FOLD 3...\n",
      "-- FOLD 4...\n",
      "-- FOLD 5...\n",
      "-- FOLD 6...\n",
      "-- FOLD 7...\n",
      "-- FOLD 8...\n",
      "-- FOLD 9...\n",
      "\n",
      "\n",
      "Finished in 0.03 minutes\n"
     ]
    }
   ],
   "source": [
    "##### MODELING\n",
    "\n",
    "# timer\n",
    "cv_start = time.time()\n",
    "\n",
    "# Assuming data is in the format 'dataN' where N is the dataset number\n",
    "dataset_number = data[4:]\n",
    "\n",
    "# Create the directory path (assuming directories data1, data2, ..., data50 already exist)/\n",
    "input_dir = os.path.join(data_path, 'prepared')\n",
    "output_dir = os.path.join(res_path, 'preprocess1')\n",
    "\n",
    "# list of processors\n",
    "methods = ['RW', 'DI']\n",
    "\n",
    "# processing loop\n",
    "for m in methods:\n",
    "    \n",
    "    # feedback\n",
    "    print('-'*30)\n",
    "    print('- METHOD: ' + m + '...')\n",
    "    print('-'*30)\n",
    "\n",
    "    # loop through fold combinations\n",
    "    for fold in range(num_folds):\n",
    "    \n",
    "        ##### LOAD DATA\n",
    "        # import data subsets\n",
    "        train_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_train.pkl')\n",
    "        valid_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_valid.pkl')\n",
    "        test_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_test.pkl')\n",
    "        \n",
    "        # Load the dataset\n",
    "        with open(train_path, 'rb') as file:\n",
    "            data_train = pickle.load(file)\n",
    "        with open(valid_path, 'rb') as file:\n",
    "            data_valid = pickle.load(file)\n",
    "        with open(test_path, 'rb') as file:\n",
    "            data_test = pickle.load(file)\n",
    "            \n",
    "            \n",
    "        ##### MODELING\n",
    "    \n",
    "        # reweighing\n",
    "        if m == 'RW':\n",
    "            \n",
    "            # fit pre-processor\n",
    "            RW = Reweighing(unprivileged_groups = unprivileged_groups,\n",
    "                            privileged_groups   = privileged_groups)\n",
    "            RW.fit(data_train)\n",
    "\n",
    "            # train processing\n",
    "            dataset_transf_train = RW.transform(data_train)\n",
    "            w_train   = dataset_transf_train.instance_weights.ravel()\n",
    "            out_train = dataset_transf_train.convert_to_dataframe()[0]\n",
    "            out_train = out_train.sample(n = out_train.shape[0], replace = True, weights = w_train)\n",
    "\n",
    "            # valid classification\n",
    "            dataset_transf_valid = RW.transform(data_valid)\n",
    "            w_valid   = dataset_transf_valid.instance_weights.ravel()\n",
    "            out_valid = dataset_transf_valid.convert_to_dataframe()[0]\n",
    "            out_valid = out_valid.sample(n = out_valid.shape[0], replace = True, weights = w_valid)\n",
    "\n",
    "            # test processing\n",
    "            dataset_transf_test = RW.transform(data_test)\n",
    "            w_test   = dataset_transf_test.instance_weights.ravel()\n",
    "            out_test = dataset_transf_test.convert_to_dataframe()[0]\n",
    "            out_test = out_test.sample(n = out_test.shape[0], replace = True, weights = w_test)\n",
    "\n",
    "            # check transformation\n",
    "            assert np.abs(dataset_transf_train.instance_weights.sum() - data_train.instance_weights.sum()) < 1e-6\n",
    "\n",
    "            # check results\n",
    "            metric_transf_train = BinaryLabelDatasetMetric(dataset_transf_train, \n",
    "                                                           unprivileged_groups = unprivileged_groups,\n",
    "                                                           privileged_groups   = privileged_groups)\n",
    "            print('-- achieved a statistical parity difference between unprivileged and privileged groups = %f' % metric_transf_train.mean_difference())\n",
    "            \n",
    "            # export CSV\n",
    "            out_train.to_csv(os.path.join(output_dir, data + '_' +  str(fold) + '_pre_train_' + m + '.csv'), index = None, header = True)\n",
    "            out_valid.to_csv(os.path.join(output_dir, data + '_' + str(fold) + '_pre_valid_' + m + '.csv'), index = None, header = True)\n",
    "            out_test.to_csv(os.path.join(output_dir, data + '_' + str(fold) + '_pre_test_'  + m + '.csv'), index = None, header = True)\n",
    "        \n",
    "\n",
    "        # disparate impact remover\n",
    "        if m == 'DI':     \n",
    "            \n",
    "            # feedback\n",
    "            print('-- FOLD ' + str(fold) + '...')\n",
    "            \n",
    "            # loop through different repair levels\n",
    "            for i in all_lambda:\n",
    "                \n",
    "                # fit pre-processor\n",
    "                di = DisparateImpactRemover(repair_level = i, sensitive_attribute = protected)\n",
    "                di.fit(data_train)\n",
    "                                          \n",
    "                # train processing\n",
    "                dataset_transf_train = di.fit_transform(data_train)\n",
    "                out_train            = dataset_transf_train.convert_to_dataframe()[0]\n",
    "\n",
    "                # valid processing\n",
    "                dataset_transf_valid = di.fit_transform(data_valid)\n",
    "                out_valid            = dataset_transf_valid.convert_to_dataframe()[0]\n",
    "\n",
    "                # test processing\n",
    "                dataset_transf_test = di.fit_transform(data_test)\n",
    "                out_test            = dataset_transf_test.convert_to_dataframe()[0]\n",
    "               \n",
    "                # export CSV\n",
    "                out_train.to_csv(os.path.join(output_dir, data + '_' + str(fold) + '_pre_train_' + m + '_' + str(i) + '.csv'), index = None, header = True)\n",
    "                out_valid.to_csv(os.path.join(output_dir, data + '_' + str(fold) + '_pre_valid_' + m + '_' + str(i) + '.csv'), index = None, header = True)\n",
    "                out_test.to_csv(os.path.join(output_dir, data + '_' + str(fold) + '_pre_test_'  + m + '_' + str(i) + '.csv'), index = None, header = True)\n",
    "                                 \n",
    "    # feedback\n",
    "    print('')\n",
    "    \n",
    "# print performance\n",
    "print('')\n",
    "print('Finished in {:.2f} minutes'.format((time.time() - cv_start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
