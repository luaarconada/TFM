{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAIR IN-PROCESSING\n",
    "\n",
    "This notebook implements the Adversarial Debiasing in-processor [(Zhang et al. 2018)](https://dl.acm.org/doi/abs/10.1145/3278721.3278779).\n",
    "\n",
    "The modeling is performed separately for each combination of training folds. This is controlled with `use_fold` variable. To fit adversarial debiasing on a different combination of training folds, set `use_fold` to a specific value and restar the kernel.\n",
    "\n",
    "A further analysis of the processor outputs is performed in `code_05_inprocess3.R`.\n",
    "\n",
    "The notebook loads the data exported in `code_00_partitinoing.ipynb` and applies in-processors. The processor predictions are exported as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "source": [
    "##### PACKAGES\n",
    "\n",
    "# working paths\n",
    "%run code_00_working_paths.py\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(func_path)\n",
    "\n",
    "from load_data import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameters and preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PARAMETERS\n",
    "\n",
    "# specify data set\n",
    "# one of ['data1', 'data2', ..., 'data50']\n",
    "data = 'data50'\n",
    "\n",
    "# partitioning\n",
    "num_folds = 5\n",
    "use_fold  = 3 # one of [0, 1, ..., 4 (num_folds-1)]\n",
    "seed      = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### IN-PROCESSOR PARAMS\n",
    "\n",
    "adversary_loss_weight = 0.1 # other options: [0.1, 0.01, 0.001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RANDOM SEED\n",
    "\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 6)\n"
     ]
    }
   ],
   "source": [
    "##### LOAD PARTITIONING\n",
    "\n",
    "# Assuming data is in the format 'dataN' where N is the dataset number\n",
    "dataset_number = data[4:]\n",
    "\n",
    "# Create the directory path (assuming directories data1, data2, ..., data50 already exist)\n",
    "input_dir = os.path.join(data_path, 'prepared', 'data' + dataset_number)\n",
    "\n",
    "# Construct the full file path\n",
    "file_path = os.path.join(input_dir, data + '_orig_test.pkl')\n",
    "\n",
    "# Load the dataset\n",
    "with open(file_path, 'rb') as file:\n",
    "    dataset_orig_test = pickle.load(file)\n",
    "    \n",
    "# Convert to dataframe and print the shape\n",
    "te = dataset_orig_test.convert_to_dataframe()[0]\n",
    "print(te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DATA PREP\n",
    "\n",
    "# protected attribute\n",
    "protected           = 'race'\n",
    "privileged_groups   = [{'race': 1}] \n",
    "unprivileged_groups = [{'race': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fair processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_14176\\3867462334.py:3: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_14176\\3867462334.py:3: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_14176\\3867462334.py:6: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_14176\\3867462334.py:6: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "- FOLD 3...\n",
      "------------------------------\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:164: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:164: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.674779; batch adversarial loss: 0.898208\n",
      "epoch 1; iter: 0; batch classifier loss: 0.674481; batch adversarial loss: 0.877523\n",
      "epoch 2; iter: 0; batch classifier loss: 0.661982; batch adversarial loss: 0.877320\n",
      "epoch 3; iter: 0; batch classifier loss: 0.691708; batch adversarial loss: 0.906067\n",
      "epoch 4; iter: 0; batch classifier loss: 0.674207; batch adversarial loss: 0.889059\n",
      "epoch 5; iter: 0; batch classifier loss: 0.632771; batch adversarial loss: 0.888548\n",
      "epoch 6; iter: 0; batch classifier loss: 0.643925; batch adversarial loss: 0.891094\n",
      "epoch 7; iter: 0; batch classifier loss: 0.644832; batch adversarial loss: 0.902236\n",
      "epoch 8; iter: 0; batch classifier loss: 0.643697; batch adversarial loss: 0.901287\n",
      "epoch 9; iter: 0; batch classifier loss: 0.642413; batch adversarial loss: 0.894101\n",
      "epoch 10; iter: 0; batch classifier loss: 0.652544; batch adversarial loss: 0.889762\n",
      "epoch 11; iter: 0; batch classifier loss: 0.739500; batch adversarial loss: 0.913834\n",
      "epoch 12; iter: 0; batch classifier loss: 0.726051; batch adversarial loss: 0.904557\n",
      "epoch 13; iter: 0; batch classifier loss: 0.680323; batch adversarial loss: 0.897422\n",
      "epoch 14; iter: 0; batch classifier loss: 0.723626; batch adversarial loss: 0.898773\n",
      "epoch 15; iter: 0; batch classifier loss: 0.717340; batch adversarial loss: 0.896830\n",
      "epoch 16; iter: 0; batch classifier loss: 0.723962; batch adversarial loss: 0.900285\n",
      "epoch 17; iter: 0; batch classifier loss: 0.764234; batch adversarial loss: 0.917915\n",
      "epoch 18; iter: 0; batch classifier loss: 0.753942; batch adversarial loss: 0.920877\n",
      "epoch 19; iter: 0; batch classifier loss: 0.769545; batch adversarial loss: 0.908855\n",
      "epoch 20; iter: 0; batch classifier loss: 0.711736; batch adversarial loss: 0.888636\n",
      "epoch 21; iter: 0; batch classifier loss: 0.814348; batch adversarial loss: 0.922146\n",
      "epoch 22; iter: 0; batch classifier loss: 0.774097; batch adversarial loss: 0.908028\n",
      "epoch 23; iter: 0; batch classifier loss: 0.884329; batch adversarial loss: 0.911520\n",
      "epoch 24; iter: 0; batch classifier loss: 0.792694; batch adversarial loss: 0.895905\n",
      "epoch 25; iter: 0; batch classifier loss: 0.723990; batch adversarial loss: 0.881368\n",
      "epoch 26; iter: 0; batch classifier loss: 0.827706; batch adversarial loss: 0.888966\n",
      "epoch 27; iter: 0; batch classifier loss: 0.930022; batch adversarial loss: 0.893681\n",
      "epoch 28; iter: 0; batch classifier loss: 0.906519; batch adversarial loss: 0.885872\n",
      "epoch 29; iter: 0; batch classifier loss: 0.797204; batch adversarial loss: 0.879719\n",
      "epoch 30; iter: 0; batch classifier loss: 0.886159; batch adversarial loss: 0.885540\n",
      "epoch 31; iter: 0; batch classifier loss: 0.953213; batch adversarial loss: 0.891054\n",
      "epoch 32; iter: 0; batch classifier loss: 0.941991; batch adversarial loss: 0.874022\n",
      "epoch 33; iter: 0; batch classifier loss: 0.970853; batch adversarial loss: 0.872545\n",
      "epoch 34; iter: 0; batch classifier loss: 0.968030; batch adversarial loss: 0.871804\n",
      "epoch 35; iter: 0; batch classifier loss: 1.099925; batch adversarial loss: 0.882068\n",
      "epoch 36; iter: 0; batch classifier loss: 1.036054; batch adversarial loss: 0.879228\n",
      "epoch 37; iter: 0; batch classifier loss: 1.022840; batch adversarial loss: 0.857209\n",
      "epoch 38; iter: 0; batch classifier loss: 0.977648; batch adversarial loss: 0.850410\n",
      "epoch 39; iter: 0; batch classifier loss: 0.990631; batch adversarial loss: 0.846391\n",
      "epoch 40; iter: 0; batch classifier loss: 1.017943; batch adversarial loss: 0.851608\n",
      "epoch 41; iter: 0; batch classifier loss: 0.962669; batch adversarial loss: 0.825956\n",
      "epoch 42; iter: 0; batch classifier loss: 0.945567; batch adversarial loss: 0.832807\n",
      "epoch 43; iter: 0; batch classifier loss: 0.969147; batch adversarial loss: 0.835632\n",
      "epoch 44; iter: 0; batch classifier loss: 1.007256; batch adversarial loss: 0.831532\n",
      "epoch 45; iter: 0; batch classifier loss: 1.130465; batch adversarial loss: 0.839099\n",
      "epoch 46; iter: 0; batch classifier loss: 1.086783; batch adversarial loss: 0.835080\n",
      "epoch 47; iter: 0; batch classifier loss: 0.998452; batch adversarial loss: 0.817388\n",
      "epoch 48; iter: 0; batch classifier loss: 1.049774; batch adversarial loss: 0.816008\n",
      "epoch 49; iter: 0; batch classifier loss: 0.902224; batch adversarial loss: 0.807986\n",
      "\n",
      "Finished in 0.06 minutes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Disable TensorFlow eager execution\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Create a TensorFlow session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Define the normalization function\n",
    "def safe_normalize(x):\n",
    "    norm = tf.norm(x)\n",
    "    print(\"x:\", x)\n",
    "    print(\"norm:\", norm)\n",
    "    return tf.cond(tf.not_equal(norm, 0), lambda: x / norm, lambda: x)\n",
    "\n",
    "##### MODELING\n",
    "\n",
    "# timer\n",
    "cv_start = time.time()\n",
    "\n",
    "# Create the directory path (assuming directories data1, data2, ..., data50 already exist)\n",
    "input_dir = os.path.join(data_path, 'prepared', 'data' + dataset_number)\n",
    "output_dir = os.path.join(res_path, 'inprocess2', 'intermediate', 'data' + dataset_number)\n",
    "\n",
    "# loop through training folds\n",
    "for fold in range(num_folds):\n",
    "    \n",
    "    ##### LOAD DATA\n",
    "    \n",
    "    # select fold combination\n",
    "    if fold != use_fold:\n",
    "        continue\n",
    "\n",
    "    # feedback\n",
    "    print('-'*30)\n",
    "    print('- FOLD ' + str(fold) + '...')\n",
    "    print('-'*30)\n",
    "\n",
    "    # import data subsets\n",
    "    train_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_train.pkl')\n",
    "    valid_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_valid.pkl')\n",
    "    test_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_test.pkl')\n",
    "        \n",
    "    # Load the dataset\n",
    "    with open(train_path, 'rb') as file:\n",
    "        data_train = pickle.load(file)\n",
    "    with open(valid_path, 'rb') as file:\n",
    "        data_valid = pickle.load(file)\n",
    "    with open(test_path, 'rb') as file:\n",
    "        data_test = pickle.load(file)\n",
    "\n",
    "    ##### MODELING\n",
    "\n",
    "    # fit adversarial debiasing\n",
    "    with tf.variable_scope('debiased_classifier', reuse=tf.AUTO_REUSE):\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups     = privileged_groups,\n",
    "                                              unprivileged_groups   = unprivileged_groups,\n",
    "                                              debias                = True,\n",
    "                                              adversary_loss_weight = adversary_loss_weight,\n",
    "                                              scope_name            = 'debiased_classifier',\n",
    "                                              sess                  = sess)\n",
    "        debiased_model.fit(data_train)\n",
    "    \n",
    "    # apply the model to valid data\n",
    "    scores_valid = debiased_model.predict(data_valid).scores\n",
    "    # Flatten scores_valid to 1D\n",
    "    scores_valid_flat = scores_valid.flatten()\n",
    "    advdebias_predictions = pd.DataFrame()\n",
    "    advdebias_predictions['scores'] = scores_valid_flat\n",
    "    advdebias_predictions['targets'] = data_valid.labels.flatten()\n",
    "    advdebias_predictions.to_csv(os.path.join(output_dir, data + '_' + str(fold) + '_AD_' + str(adversary_loss_weight) + str(use_fold) + '_predictions_valid.csv'), \n",
    "                                 index=None, \n",
    "                                 header=True)\n",
    "\n",
    "    # Apply the model to test data\n",
    "    scores_test = debiased_model.predict(data_test).scores\n",
    "    scores_test_flat = scores_test.flatten()\n",
    "\n",
    "    advdebias_predictions = pd.DataFrame()\n",
    "    advdebias_predictions['scores'] = scores_test_flat\n",
    "    advdebias_predictions.to_csv(os.path.join(output_dir, data + '_' + str(fold) + '_AD_' + str(adversary_loss_weight) + str(use_fold) + '_predictions_test.csv'), \n",
    "                                 index=None, \n",
    "                                 header=True)\n",
    "\n",
    "    # print performance\n",
    "    print('')\n",
    "    print('Finished in {:.2f} minutes'.format((time.time() - cv_start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
