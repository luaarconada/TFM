{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PARTITIONING\n",
    "\n",
    "Given a raw data set, this notebook performs the following operations:\n",
    "- randomly partitons a raw data set into training and test set\n",
    "- further splits the training set into training and validation fold combinations\n",
    "- applies `MaxAbsScaler` and exports scaled and raw data as CSV and pickle files\n",
    "\n",
    "This ensures that fairness processors trained in other notebooks use the same data partititioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameters and preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "source": [
    "##### IMPORTS\n",
    "\n",
    "# working paths\n",
    "%run code_00_working_paths.py\n",
    "\n",
    "import sys\n",
    "sys.path.append(func_path)\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import os\n",
    "\n",
    "from load_data import load_dataset\n",
    "\n",
    "from aif360.metrics import BinaryLabelDatasetMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PARAMETERS\n",
    "\n",
    "# specify data set\n",
    "# one of ['data1', 'data2', ..., 'data50']\n",
    "data = 'german'\n",
    "\n",
    "# partitioning\n",
    "test_ratio = 0.3\n",
    "num_folds  = 10\n",
    "seed       = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dataset 'german' is not recognized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m##### DATA IMPORT\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m dataset_orig \u001b[38;5;241m=\u001b[39m load_dataset(path \u001b[38;5;241m=\u001b[39m data_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m data \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      4\u001b[0m                             data \u001b[38;5;241m=\u001b[39m data)\n",
      "File \u001b[1;32mC:\\Users/Usuario/Downloads/TFM/Code/functions\\load_data.py:16\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, data)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Imports and prepares dataset with a generic preprocessing function'''\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatacorr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data[\u001b[38;5;241m8\u001b[39m:]\u001b[38;5;241m.\u001b[39misdigit() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(data[\u001b[38;5;241m8\u001b[39m:]) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not recognized.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m load_data_generic(path)\n",
      "\u001b[1;31mValueError\u001b[0m: Dataset 'german' is not recognized."
     ]
    }
   ],
   "source": [
    "##### DATA IMPORT\n",
    "\n",
    "dataset_orig = load_dataset(path = data_path + 'raw/' + data + '.csv', \n",
    "                            data = data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 6)\n"
     ]
    }
   ],
   "source": [
    "##### DATA PREP\n",
    "\n",
    "# protected attribute\n",
    "protected           = 'race'\n",
    "privileged_groups   = [{'race': 1}] \n",
    "unprivileged_groups = [{'race': 0}]\n",
    "\n",
    "# check dimensions\n",
    "print(dataset_orig.metadata['params']['df'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     priors  sentence  race  rehab=Not-participated  rehab=Participated  \\\n",
      "0  1.026650  3.155870   1.0                     1.0                 0.0   \n",
      "1  0.477052  1.551613   1.0                     0.0                 1.0   \n",
      "2  1.031786  4.398357   1.0                     0.0                 1.0   \n",
      "3  0.913128  2.880531   1.0                     1.0                 0.0   \n",
      "4  1.233794  3.899129   1.0                     0.0                 1.0   \n",
      "\n",
      "   target  \n",
      "0     2.0  \n",
      "1     2.0  \n",
      "2     2.0  \n",
      "3     2.0  \n",
      "4     2.0  \n"
     ]
    }
   ],
   "source": [
    "# Convert the StandardDataset to a pandas DataFrame\n",
    "df = dataset_orig.convert_to_dataframe()[0]\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Partitioning and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560, 6) (240, 6)\n"
     ]
    }
   ],
   "source": [
    "##### PARTITIONING\n",
    "\n",
    "# set seed\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Assuming data is in the format 'dataN' where N is the dataset number\n",
    "dataset_number = data[4:]\n",
    "\n",
    "# Create the directory path (assuming directories data1, data2, ..., data50 already exist)\n",
    "output_dir = os.path.join(data_path, 'prepared')\n",
    "\n",
    "# Train / test partitioning\n",
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([1 - test_ratio], shuffle=True)\n",
    "tr = dataset_orig_train.convert_to_dataframe()[0]\n",
    "te = dataset_orig_test.convert_to_dataframe()[0]\n",
    "\n",
    "# Export test set\n",
    "te.to_csv(os.path.join(output_dir, data + '_' + 'orig_test.csv'), index=None, header=True)\n",
    "pickle.dump(dataset_orig_test, open(os.path.join(output_dir, data + '_' + 'orig_test.pkl'), 'wb'))\n",
    "print(tr.shape, te.shape)\n",
    "\n",
    "# cross-validation on the training set\n",
    "skf = dataset_orig_train.split(num_or_size_splits = num_folds, seed = seed, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- (504, 6) (56, 6)\n",
      "-- (504, 6) (56, 6)\n",
      "-- (504, 6) (56, 6)\n",
      "-- (504, 6) (56, 6)\n",
      "-- (504, 6) (56, 6)\n",
      "-- (504, 6) (56, 6)\n",
      "-- (504, 6) (56, 6)\n",
      "-- (504, 6) (56, 6)\n",
      "-- (504, 6) (56, 6)\n",
      "-- (504, 6) (56, 6)\n",
      "\n",
      "Finished in 0.01 minutes\n"
     ]
    }
   ],
   "source": [
    "##### SCALING\n",
    "\n",
    "# timer\n",
    "cv_start = time.time()\n",
    "\n",
    "# data partitioning loop\n",
    "for fold in range(num_folds):\n",
    "\n",
    "    ##### DATA PARTITIONING\n",
    "\n",
    "    # validation fold\n",
    "    data_valid = skf[fold].copy()\n",
    "\n",
    "    # train folds\n",
    "    train_folds = [f for f in range(num_folds) if f != fold]\n",
    "    for fold_idx in train_folds:\n",
    "\n",
    "        if fold_idx == train_folds[0]:\n",
    "            data_train = skf[fold_idx].copy()\n",
    "        else:\n",
    "            data_train.features             = np.concatenate([data_train.features, skf[fold_idx].features],                         axis = 0)\n",
    "            data_train.instance_names       = np.concatenate([data_train.instance_names, skf[fold_idx].instance_names],             axis = 0)\n",
    "            data_train.instance_weights     = np.concatenate([data_train.instance_weights, skf[fold_idx].instance_weights],         axis = 0)\n",
    "            data_train.labels               = np.concatenate([data_train.labels, skf[fold_idx].labels],                             axis = 0)\n",
    "            data_train.protected_attributes = np.concatenate([data_train.protected_attributes, skf[fold_idx].protected_attributes], axis = 0)\n",
    "            data_train.scores               = np.concatenate([data_train.scores, skf[fold_idx].scores],                             axis = 0)\n",
    "\n",
    "    # test set\n",
    "    data_test = dataset_orig_test.copy()\n",
    "    \n",
    "    # convert to DF\n",
    "    tr = data_train.convert_to_dataframe()[0]\n",
    "    va = data_valid.convert_to_dataframe()[0]\n",
    "\n",
    "    # Export CSV\n",
    "    tr.to_csv(os.path.join(output_dir, data + '_' + f'orig_{fold}_train.csv'), index=None, header=True)\n",
    "    va.to_csv(os.path.join(output_dir, data + '_' + f'orig_{fold}_valid.csv'), index=None, header=True)\n",
    "    \n",
    "    # Export pickle\n",
    "    pickle.dump(data_train, open(os.path.join(output_dir, data + '_' + f'orig_{fold}_train.pkl'), 'wb'))\n",
    "    pickle.dump(data_valid, open(os.path.join(output_dir, data + '_' + f'orig_{fold}_valid.pkl'), 'wb'))\n",
    "    print('--', tr.shape, va.shape)\n",
    "\n",
    "\n",
    "    ##### SCALING\n",
    "\n",
    "    # scale features\n",
    "    min_max_scaler      = MaxAbsScaler()\n",
    "    data_train.features = min_max_scaler.fit_transform(data_train.features)\n",
    "    data_valid.features = min_max_scaler.transform(data_valid.features)\n",
    "    data_test.features  = min_max_scaler.transform(data_test.features)\n",
    "\n",
    "    # convert to DF\n",
    "    tr = data_train.convert_to_dataframe()[0]\n",
    "    va = data_valid.convert_to_dataframe()[0]\n",
    "    te = data_test.convert_to_dataframe()[0]\n",
    "\n",
    "    # Save CSV\n",
    "    tr.to_csv(os.path.join(output_dir, data + '_' + f'scaled_{fold}_train.csv'), index=None, header=True)\n",
    "    va.to_csv(os.path.join(output_dir, data + '_' + f'scaled_{fold}_valid.csv'), index=None, header=True)\n",
    "    te.to_csv(os.path.join(output_dir, data + '_' + f'scaled_{fold}_test.csv'), index=None, header=True)\n",
    "    \n",
    "    # Save pickle\n",
    "    pickle.dump(data_train, open(os.path.join(output_dir, data + '_' + f'scaled_{fold}_train.pkl'), 'wb'))\n",
    "    pickle.dump(data_valid, open(os.path.join(output_dir, data + '_' + f'scaled_{fold}_valid.pkl'), 'wb'))\n",
    "    pickle.dump(data_test, open(os.path.join(output_dir, data + '_' + f'scaled_{fold}_test.pkl'), 'wb'))\n",
    "\n",
    "# print performance\n",
    "print('')\n",
    "print('Finished in {:.2f} minutes'.format((time.time() - cv_start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
