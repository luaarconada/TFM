{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b262ebd5",
   "metadata": {},
   "source": [
    "# Pre1 + post1 + post4\n",
    "1. Reweighting + Reject Option Classification\n",
    "2. Disparate Impact Remover + Reject Option Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f9292cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PACKAGES\n",
    "\n",
    "# working paths\n",
    "%run code_00_working_paths.py\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.preprocessing.reweighing import Reweighing\n",
    "from aif360.algorithms.preprocessing.lfr import LFR\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "from aif360.algorithms.postprocessing.reject_option_classification import RejectOptionClassification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(func_path)\n",
    "\n",
    "from load_data import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c6eb35",
   "metadata": {},
   "source": [
    "## Parameters and preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8350053",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PARAMETERS\n",
    "\n",
    "# specify data set\n",
    "data = 'datacorr1'\n",
    "\n",
    "# partitioning\n",
    "num_folds = 10\n",
    "seed      = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e4b8947",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PRE-PROCESSOR PARAMS\n",
    "\n",
    "all_lambda = [0.5,0.6,0.7,0.8,0.9,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "134531ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### POST-PROCESSOR PARAMS\n",
    "\n",
    "metric_name      = 'Statistical parity difference'\n",
    "num_class_thresh = 100\n",
    "num_ROC_margin   = 50\n",
    "all_bound        = [0.1, 0.2, 0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f560c55",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "631b1f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RANDOM SEED\n",
    "\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d750f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 6)\n"
     ]
    }
   ],
   "source": [
    "##### LOAD PARTITIONING\n",
    "\n",
    "# Assuming data is in the format 'dataN' where N is the dataset number\n",
    "dataset_number = data[4:]\n",
    "\n",
    "# Create the directory path (assuming directories data1, data2, ..., data50 already exist)\n",
    "input_dir = os.path.join(data_path, 'prepared')\n",
    "\n",
    "# Construct the full file path\n",
    "file_path = os.path.join(input_dir, data + '_orig_test.pkl')\n",
    "\n",
    "# Load the dataset\n",
    "with open(file_path, 'rb') as file:\n",
    "    dataset_orig_test = pickle.load(file)\n",
    "    \n",
    "# Convert to dataframe and print the shape\n",
    "te = dataset_orig_test.convert_to_dataframe()[0]\n",
    "print(te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a479c7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DATA PREP\n",
    "# protected attribute\n",
    "protected           = 'race'\n",
    "privileged_groups   = [{'race': 1}] \n",
    "unprivileged_groups = [{'race': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d3f9a8",
   "metadata": {},
   "source": [
    "## Fair processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11bf9e1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "- FOLD 0...\n",
      "------------------------------\n",
      "-- BOUND 0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\postprocessing\\reject_option_classification.py:160: UserWarning: Unable to satisy fairness constraints\n",
      "  warn(\"Unable to satisy fairness constraints\")\n",
      "C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\postprocessing\\reject_option_classification.py:160: UserWarning: Unable to satisy fairness constraints\n",
      "  warn(\"Unable to satisy fairness constraints\")\n",
      "C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\postprocessing\\reject_option_classification.py:160: UserWarning: Unable to satisy fairness constraints\n",
      "  warn(\"Unable to satisy fairness constraints\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- BOUND 0.2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\postprocessing\\reject_option_classification.py:160: UserWarning: Unable to satisy fairness constraints\n",
      "  warn(\"Unable to satisy fairness constraints\")\n",
      "C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\postprocessing\\reject_option_classification.py:160: UserWarning: Unable to satisy fairness constraints\n",
      "  warn(\"Unable to satisy fairness constraints\")\n",
      "C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\postprocessing\\reject_option_classification.py:160: UserWarning: Unable to satisy fairness constraints\n",
      "  warn(\"Unable to satisy fairness constraints\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- BOUND 0.3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\postprocessing\\reject_option_classification.py:160: UserWarning: Unable to satisy fairness constraints\n",
      "  warn(\"Unable to satisy fairness constraints\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Finished in 0.85 minutes\n"
     ]
    }
   ],
   "source": [
    "# RW + ROC\n",
    "##### MODELING\n",
    "\n",
    "# timer\n",
    "cv_start = time.time()\n",
    "\n",
    "# Create the directory path (assuming directories data1, data2, ..., data50 already exist)\n",
    "input_dir = os.path.join(data_path, 'prepared')\n",
    "output_dir = os.path.join(res_path, 'in2post1post4')\n",
    "\n",
    "# base models\n",
    "model_names = ['glm', \"rf\", \"nnet\"]\n",
    "\n",
    "# loop through folds\n",
    "#for fold in range(num_folds):\n",
    "\n",
    "fold = 0\n",
    "\n",
    "##### LOAD DATA\n",
    "\n",
    "# feedback\n",
    "print('-'*30)\n",
    "print('- FOLD ' + str(fold) + '...')\n",
    "print('-'*30)\n",
    "\n",
    "# import data subsets\n",
    "train_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_train.pkl')\n",
    "valid_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_valid.pkl')\n",
    "test_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_test.pkl')\n",
    "\n",
    "# Load the dataset\n",
    "with open(train_path, 'rb') as file:\n",
    "    data_train = pickle.load(file)\n",
    "with open(valid_path, 'rb') as file:\n",
    "    data_valid = pickle.load(file)\n",
    "with open(test_path, 'rb') as file:\n",
    "    data_test = pickle.load(file)\n",
    "\n",
    "\n",
    "##### MODELING\n",
    "\n",
    "# import prediction results from R\n",
    "dataset_trainResults_valid = pd.read_csv(res_path + 'in2post1/' + data + '_' + str(fold) + '_AD_POST_training_results_dval.csv')\n",
    "dataset_trainResults_test  = pd.read_csv(res_path + 'in2post1/' + data + '_' + str(fold) + '_AD_POST_training_results_dtest.csv')\n",
    "\n",
    "# Convert to DataFrame\n",
    "data_valid_df = data_valid.convert_to_dataframe()[0]\n",
    "data_test_df = data_test.convert_to_dataframe()[0]\n",
    "\n",
    "# loop through bound values\n",
    "for i in all_bound:\n",
    "\n",
    "    # feedback\n",
    "    print('-- BOUND ' + str(i) + '...')\n",
    "\n",
    "    # placeholder\n",
    "    ROC_test = pd.DataFrame()\n",
    "\n",
    "    # loop through base classifiers\n",
    "    for m in model_names:\n",
    "\n",
    "        # extract validation preds\n",
    "        scores_valid = np.array(dataset_trainResults_valid[m + '_scores']).reshape(len(dataset_trainResults_valid.index),1)\n",
    "        labels_valid = np.where(dataset_trainResults_valid[m + '_class'] == 'Parole', 1.0, 2.0).reshape(len(dataset_trainResults_valid.index), 1)\n",
    "\n",
    "        # extract test preds\n",
    "        scores_test = np.array(dataset_trainResults_test[m + '_scores']).reshape(len(dataset_trainResults_test.index),1)\n",
    "        labels_test = np.where(dataset_trainResults_test[m + '_class'] == 'Parole', 1.0, 2.0).reshape(len(dataset_trainResults_test.index), 1)\n",
    "\n",
    "        # write predictions to DataFrame\n",
    "        data_valid_df['scores'] = scores_valid\n",
    "        data_valid_df['labels'] = labels_valid\n",
    "        data_test_df['scores'] = scores_test\n",
    "        data_test_df['labels'] = labels_test\n",
    "\n",
    "        # replace 'race' with the actual column name for your protected attribute\n",
    "        protected_attribute_name = 'race'  # replace with your actual column name\n",
    "\n",
    "        # create BinaryLabelDataset\n",
    "        dataset_valid_bl = BinaryLabelDataset(df=data_valid_df,\n",
    "                                              label_names=['labels'],\n",
    "                                              protected_attribute_names=[protected_attribute_name],  \n",
    "                                              favorable_label=1.0,\n",
    "                                              unfavorable_label=2.0)\n",
    "        dataset_test_bl = BinaryLabelDataset(df=data_test_df,\n",
    "                                             label_names=['labels'],\n",
    "                                             protected_attribute_names=[protected_attribute_name],  \n",
    "                                             favorable_label=1.0,\n",
    "                                             unfavorable_label=2.0)\n",
    "\n",
    "        # fit ROC\n",
    "        ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
    "                                         privileged_groups=privileged_groups, \n",
    "                                         num_class_thresh=num_class_thresh, \n",
    "                                         num_ROC_margin=num_ROC_margin,\n",
    "                                         metric_name=metric_name,\n",
    "                                         metric_ub=i, \n",
    "                                         metric_lb=-i)\n",
    "        ROC = ROC.fit(dataset_valid_bl, dataset_valid_bl)\n",
    "\n",
    "        # predict test scores\n",
    "        dataset_transf_test_pred = ROC.predict(dataset_test_bl)\n",
    "        ROC_test[m + \"_fairScores\"] = dataset_transf_test_pred.scores.flatten()\n",
    "        label_names = np.where(dataset_transf_test_pred.labels == 1, 'Parole', 'Noparole')\n",
    "        ROC_test[m + \"_fairLabels\"] = label_names\n",
    "\n",
    "    # export CSV\n",
    "    ROC_test.to_csv(os.path.join(output_dir, data + '_' + str(fold) + '_ROC_' + str(i) + '_AD_predictions_test.csv'),  index=None, header=True)\n",
    "\n",
    "# feedback\n",
    "print('')\n",
    "\n",
    "# print performance\n",
    "print('')\n",
    "print('Finished in {:.2f} minutes'.format((time.time() - cv_start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff863df1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DI + ROC\n",
    "##### MODELING\n",
    "\n",
    "# timer\n",
    "cv_start = time.time()\n",
    "\n",
    "# Create the directory path (assuming directories data1, data2, ..., data50 already exist)\n",
    "input_dir = os.path.join(data_path, 'prepared/')\n",
    "output_dir = os.path.join(res_path, 'pre1post1post4')\n",
    "\n",
    "# base models\n",
    "model_names = ['glm', 'rf', 'nnet']\n",
    "\n",
    "# Loop through folds\n",
    "for fold in range(num_folds):\n",
    "    \n",
    "    # Feedback\n",
    "    print('-'*30)\n",
    "    print('- FOLD ' + str(fold) + '...')\n",
    "    print('-'*30)\n",
    "\n",
    "    # import data subsets\n",
    "    train_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_train.pkl')\n",
    "    valid_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_valid.pkl')\n",
    "    test_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_test.pkl')\n",
    "\n",
    "    # Load the dataset\n",
    "    with open(train_path, 'rb') as file:\n",
    "        data_train = pickle.load(file)\n",
    "    with open(valid_path, 'rb') as file:\n",
    "        data_valid = pickle.load(file)\n",
    "    with open(test_path, 'rb') as file:\n",
    "        data_test = pickle.load(file)\n",
    "\n",
    "    # Import prediction results from R\n",
    "    dataset_trainResults_valid = pd.read_csv(res_path + 'pre1post1/' + data + '_' + str(fold) + '_POST_training_results_dval.csv')\n",
    "    dataset_trainResults_test = pd.read_csv(res_path + 'pre1post1/' + data + '_' + str(fold) + '_POST_training_results_dtest.csv')\n",
    "\n",
    "    # Extract underlying DataFrame\n",
    "    data_valid_df = data_valid.convert_to_dataframe()[0]\n",
    "    data_test_df = data_test.convert_to_dataframe()[0]\n",
    "\n",
    "    # Ensure 'protected_attribute' column is present\n",
    "    if 'protected_attribute' not in data_valid_df.columns:\n",
    "        data_valid_df['protected_attribute'] = data_valid.protected_attributes[:, 0]\n",
    "    if 'protected_attribute' not in data_test_df.columns:\n",
    "        data_test_df['protected_attribute'] = data_test.protected_attributes[:, 0]\n",
    "\n",
    "    # Ensure 'race' is the actual column name of the protected attribute\n",
    "    protected_attribute_name = 'race'  # Replace with your actual column name if different\n",
    "\n",
    "    if protected_attribute_name not in data_valid_df.columns:\n",
    "        data_valid_df[protected_attribute_name] = data_valid.protected_attributes[:, 0]\n",
    "    if protected_attribute_name not in data_test_df.columns:\n",
    "        data_test_df[protected_attribute_name] = data_test.protected_attributes[:, 0]\n",
    "\n",
    "    # Loop through bound values\n",
    "    for i in all_bound:\n",
    "\n",
    "        # Feedback\n",
    "        print('-- BOUND ' + str(i) + '...')\n",
    "\n",
    "        # Placeholder\n",
    "        ROC_test = pd.DataFrame()\n",
    "\n",
    "        # Loop through base classifiers\n",
    "        for m in model_names:\n",
    "\n",
    "            # Extract validation preds\n",
    "            scores_valid = np.array(dataset_trainResults_valid[m + '_scores']).reshape(len(dataset_trainResults_valid.index), 1)\n",
    "            labels_valid = np.where(dataset_trainResults_valid[m + '_class'] == 'Parole', 1.0, 0.0).reshape(len(dataset_trainResults_valid.index), 1)\n",
    "\n",
    "            # Extract test preds\n",
    "            scores_test = np.array(dataset_trainResults_test[m + '_scores']).reshape(len(dataset_trainResults_test.index), 1)\n",
    "            labels_test = np.where(dataset_trainResults_test[m + '_class'] == 'Parole', 1.0, 0.0).reshape(len(dataset_trainResults_test.index), 1)\n",
    "\n",
    "            # Add predictions as columns to DataFrame\n",
    "            data_valid_df['scores'] = scores_valid\n",
    "            data_valid_df['labels'] = labels_valid\n",
    "            data_test_df['scores'] = scores_test\n",
    "            data_test_df['labels'] = labels_test\n",
    "\n",
    "            # Create BinaryLabelDataset\n",
    "            dataset_valid_bl = BinaryLabelDataset(df=data_valid_df,\n",
    "                                                  label_names=['labels'],\n",
    "                                                  protected_attribute_names=[protected_attribute_name],\n",
    "                                                  favorable_label=1.0,\n",
    "                                                  unfavorable_label=0.0)\n",
    "            dataset_test_bl = BinaryLabelDataset(df=data_test_df,\n",
    "                                                 label_names=['labels'],\n",
    "                                                 protected_attribute_names=[protected_attribute_name],\n",
    "                                                 favorable_label=1.0,\n",
    "                                                 unfavorable_label=0.0)\n",
    "\n",
    "            # Fit ROC with error handling\n",
    "            try:\n",
    "                ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups,\n",
    "                                                 num_class_thresh=num_class_thresh,\n",
    "                                                 num_ROC_margin=num_ROC_margin,\n",
    "                                                 metric_name=metric_name,\n",
    "                                                 metric_ub=i,\n",
    "                                                 metric_lb=-i)\n",
    "                ROC = ROC.fit(dataset_valid_bl, dataset_valid_bl)\n",
    "                \n",
    "                # Predict test scores\n",
    "                dataset_transf_test_pred = ROC.predict(dataset_test_bl)\n",
    "                ROC_test[m + \"_fairScores\"] = dataset_transf_test_pred.scores.flatten()\n",
    "                label_names = np.where(dataset_transf_test_pred.labels == 1, 'Parole', 'NoParole')\n",
    "                ROC_test[m + \"_fairLabels\"] = label_names\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error for fold {fold}, bound {i}, model {m}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Export CSV\n",
    "        ROC_test.to_csv(os.path.join(output_dir, data + '_' + str(fold) + '_ROC_' + str(i) + '_predictions_test.csv'), index=None, header=True)\n",
    "\n",
    "    # Feedback\n",
    "    print('')\n",
    "\n",
    "# Print performance\n",
    "print('')\n",
    "print('Finished in {:.2f} minutes'.format((time.time() - cv_start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0631246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
