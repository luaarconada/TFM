{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAIR IN-PROCESSING\n",
    "\n",
    "This notebook implements the following in-processors:\n",
    "- Prejudice Remover [(Kamishima et al. 2012)](https://link.springer.com/chapter/10.1007/978-3-642-33486-3_3)\n",
    "\n",
    "A further analysis of the processor outputs is performed in `code_06_inprocess3.R`.\n",
    "\n",
    "The notebook loads the data exported in `code_00_partitioning.ipynb` and applies in-processors. The processor predictions are exported as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PACKAGES\n",
    "\n",
    "# working paths\n",
    "%run code_00_working_paths.py\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(func_path)\n",
    "\n",
    "from load_data import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameters and preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PARAMETERS\n",
    "\n",
    "# sepcify data set\n",
    "# one of ['data1', 'data2', ..., 'data50']\n",
    "data = 'data1'\n",
    "\n",
    "# partitioning\n",
    "num_folds  = 10\n",
    "seed       = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### IN-PROCESSOR PARAMS\n",
    "\n",
    "all_eta = [1, 15, 50, 70, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RANDOM SEED\n",
    "\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 6)\n"
     ]
    }
   ],
   "source": [
    "# Assuming data is in the format 'dataN' where N is the dataset number\n",
    "dataset_number = data[4:]\n",
    "\n",
    "# Create the directory path (assuming directories data1, data2, ..., data50 already exist)\n",
    "input_dir = os.path.join(data_path, 'prepared')\n",
    "\n",
    "# Construct the full file path\n",
    "file_path = os.path.join(input_dir, data + '_orig_test.pkl')\n",
    "\n",
    "# Load the dataset\n",
    "with open(file_path, 'rb') as file:\n",
    "    dataset_orig_test = pickle.load(file)\n",
    "    \n",
    "# Convert to dataframe and print the shape\n",
    "te = dataset_orig_test.convert_to_dataframe()[0]\n",
    "print(te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DATA PREP\n",
    "\n",
    "# protected attribute\n",
    "protected           = 'race'\n",
    "privileged_groups   = [{'race': 1}] \n",
    "unprivileged_groups = [{'race': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fair processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "- FOLD 0...\n",
      "------------------------------\n",
      "--- eta: 1.00\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# fit PR\u001b[39;00m\n\u001b[0;32m     52\u001b[0m debiased_model \u001b[38;5;241m=\u001b[39m PrejudiceRemover(eta \u001b[38;5;241m=\u001b[39m eta, sensitive_attr \u001b[38;5;241m=\u001b[39m protected, class_attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 53\u001b[0m debiased_model\u001b[38;5;241m.\u001b[39mfit(data_train)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# predict validation scores\u001b[39;00m\n\u001b[0;32m     56\u001b[0m dataset_debiasing_valid       \u001b[38;5;241m=\u001b[39m debiased_model\u001b[38;5;241m.\u001b[39mpredict(data_valid)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\transformer.py:27\u001b[0m, in \u001b[0;36maddmetadata.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 27\u001b[0m     new_dataset \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_dataset, Dataset):\n\u001b[0;32m     29\u001b[0m         new_dataset\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m new_dataset\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\prejudice_remover.py:131\u001b[0m, in \u001b[0;36mPrejudiceRemover.fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Learns the regularized logistic regression model.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m        PrejudiceRemover: Returns self.\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack([dataset\u001b[38;5;241m.\u001b[39mfeatures, dataset\u001b[38;5;241m.\u001b[39mlabels])\n\u001b[0;32m    132\u001b[0m     columns \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mfeature_names \u001b[38;5;241m+\u001b[39m dataset\u001b[38;5;241m.\u001b[39mlabel_names\n\u001b[0;32m    133\u001b[0m     train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mdata, columns\u001b[38;5;241m=\u001b[39mcolumns)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5896\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   5897\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   5898\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   5899\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5900\u001b[0m ):\n\u001b[0;32m   5901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 5902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'features'"
     ]
    }
   ],
   "source": [
    "##### MODELING: PREJUDICE REMOVER\n",
    "import pandas as pd\n",
    "\n",
    "# timer\n",
    "cv_start = time.time()\n",
    "\n",
    "# Assuming data is in the format 'dataN' where N is the dataset number\n",
    "dataset_number = data[4:]\n",
    "\n",
    "# Create the directory path (assuming directories data1, data2, ..., data50 already exist)/\n",
    "input_dir = os.path.join(data_path, 'prepared')\n",
    "output_dir = os.path.join(res_path, 'inprocess1', 'intermediate')\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(num_folds):\n",
    "    \n",
    "    ##### LOAD DATA\n",
    "\n",
    "    # feedback\n",
    "    print('-'*30)\n",
    "    print('- FOLD ' + str(fold) + '...')\n",
    "    print('-'*30)\n",
    "\n",
    "    # import data subsets\n",
    "    train_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_train.pkl')\n",
    "    valid_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_valid.pkl')\n",
    "    test_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_test.pkl')\n",
    "        \n",
    "    # Load the dataset\n",
    "    with open(train_path, 'rb') as file:\n",
    "        data_train = pickle.load(file)\n",
    "    with open(valid_path, 'rb') as file:\n",
    "        data_valid = pickle.load(file)\n",
    "    with open(test_path, 'rb') as file:\n",
    "        data_test = pickle.load(file)\n",
    "\n",
    "\n",
    "    ##### MODELING\n",
    "\n",
    "    # placeholders\n",
    "    pr_predictions_valid = pd.DataFrame()\n",
    "    pr_predictions_test  = pd.DataFrame()\n",
    "\n",
    "    # loop through eta\n",
    "    for eta in all_eta:\n",
    "        \n",
    "        # feedback\n",
    "        print('--- eta: %.2f' % eta)\n",
    "        colname = 'eta_' + str(eta)\n",
    "\n",
    "        # fit PR\n",
    "        debiased_model = PrejudiceRemover(eta = eta, sensitive_attr = protected, class_attr = 'target')\n",
    "        debiased_model.fit(data_train)\n",
    "\n",
    "        # predict validation scores\n",
    "        dataset_debiasing_valid       = debiased_model.predict(data_valid)\n",
    "        scores                        = dataset_debiasing_valid.scores\n",
    "        pr_predictions_valid[colname] = sum(scores.tolist(), [])\n",
    "\n",
    "        # predict test scores\n",
    "        dataset_debiasing_test       = debiased_model.predict(data_test)\n",
    "        scores                       = dataset_debiasing_test.scores\n",
    "        pr_predictions_test[colname] = sum(scores.tolist(), [])\n",
    "\n",
    "    # export CSV\n",
    "    pr_predictions_valid.to_csv(os.path.join(output_dir, data + '_' +  str(fold) + '_PR_predictions_valid.csv'), index = None, header=True)\n",
    "    pr_predictions_test.to_csv(os.path.join(output_dir, data + '_' + str(fold) + '_PR_predictions_test.csv'),  index = None, header=True)\n",
    "    print('')\n",
    "  \n",
    "    \n",
    "# print performance\n",
    "print('')\n",
    "print('Finished in {:.2f} minutes'.format((time.time() - cv_start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
