{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAIR IN-PROCESSING\n",
    "\n",
    "This notebook implements the following in-processors:\n",
    "- Meta-Fairness Algorithm [(Celis et al. 2019)](https://dl.acm.org/doi/abs/10.1145/3287560.3287586?casa_token=VdBhACPUHUYAAAAA:D8-vlR7Vf5QVQXyYhHB23IBjO0xrKQH64wztDghcSCUpaUwwkWeMZ2Cqu76yjLvSCVhzpjleAAnJ)\n",
    "\n",
    "A further analysis of the processor outputs is performed in `code_06_inprocess3.R`.\n",
    "\n",
    "The notebook loads the data exported in `code_00_partitinoing.ipynb` and applies in-processors. The processor predictions are exported as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "source": [
    "##### PACKAGES\n",
    "\n",
    "# working paths\n",
    "%run code_00_working_paths.py\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.inprocessing import MetaFairClassifier, PrejudiceRemover\n",
    "from aif360.algorithms.preprocessing import Reweighing, LFR, DisparateImpactRemover\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(func_path)\n",
    "\n",
    "from load_data import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameters and preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PARAMETERS\n",
    "\n",
    "# sepcify data set\n",
    "# one of ['data1', 'data2', ..., 'data50']\n",
    "data = 'data1' \n",
    "\n",
    "# partitioning\n",
    "num_folds  = 5\n",
    "seed       = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### IN-PROCESSOR PARAMS\n",
    "\n",
    "all_tau = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RANDOM SEED\n",
    "\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 6)\n"
     ]
    }
   ],
   "source": [
    "# Assuming data is in the format 'dataN' where N is the dataset number\n",
    "dataset_number = data[4:]\n",
    "\n",
    "# Create the directory path (assuming directories data1, data2, ..., data50 already exist)\n",
    "input_dir = os.path.join(data_path, 'prepared', 'data' + dataset_number)\n",
    "\n",
    "# Construct the full file path\n",
    "file_path = os.path.join(input_dir, data + '_orig_test.pkl')\n",
    "\n",
    "# Load the dataset\n",
    "with open(file_path, 'rb') as file:\n",
    "    dataset_orig_test = pickle.load(file)\n",
    "    \n",
    "# Convert to dataframe and print the shape\n",
    "te = dataset_orig_test.convert_to_dataframe()[0]\n",
    "print(te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DATA PREP\n",
    "\n",
    "# protected attribute\n",
    "protected           = 'race'\n",
    "privileged_groups   = [{'race': 1}] \n",
    "unprivileged_groups = [{'race': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fair processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "- FOLD 0...\n",
      "------------------------------\n",
      "--- tau: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:31: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_y_1 = (prob_1_1 + prob_1_0) / total\n",
      "C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:32: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_0 = (prob_m1_0 + prob_1_0) / total\n",
      "C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  prob_z_1 = (prob_m1_1 + prob_1_1) / total\n",
      "C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_0 = prob_m1_0 / total\n",
      "C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  probc_m1_1 = prob_m1_1 / total\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m debiased_model \u001b[38;5;241m=\u001b[39m MetaFairClassifier(tau \u001b[38;5;241m=\u001b[39m tau, sensitive_attr \u001b[38;5;241m=\u001b[39m protected)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     debiased_model\u001b[38;5;241m.\u001b[39mfit(data_train)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---- Error, using previous tau\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\transformer.py:27\u001b[0m, in \u001b[0;36maddmetadata.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 27\u001b[0m     new_dataset \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_dataset, Dataset):\n\u001b[0;32m     29\u001b[0m         new_dataset\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m new_dataset\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\meta_fair_classifier.py:66\u001b[0m, in \u001b[0;36mMetaFairClassifier.fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m     59\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(dataset\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;241m==\u001b[39m dataset\u001b[38;5;241m.\u001b[39mfavorable_label,\n\u001b[0;32m     60\u001b[0m                    \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     61\u001b[0m x_control_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[0;32m     62\u001b[0m         np\u001b[38;5;241m.\u001b[39misin(dataset\u001b[38;5;241m.\u001b[39mprotected_attributes[:, sens_idx],\n\u001b[0;32m     63\u001b[0m                 dataset\u001b[38;5;241m.\u001b[39mprivileged_protected_attributes[sens_idx]),\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mgetModel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau, x_train, y_train,\n\u001b[0;32m     67\u001b[0m     x_control_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\General.py:99\u001b[0m, in \u001b[0;36mGeneral.getModel\u001b[1;34m(self, tau, X, y, sens, random_state)\u001b[0m\n\u001b[0;32m     96\u001b[0m samples \u001b[38;5;241m=\u001b[39m dist_x\u001b[38;5;241m.\u001b[39mrvs(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)  \u001b[38;5;66;03m# TODO: why 20?\u001b[39;00m\n\u001b[0;32m     97\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradientDescent(dist, a, b, samples, z_1)\n\u001b[1;32m---> 99\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetValueForX(dist, a, b, params, z_1, X)\n\u001b[0;32m    100\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(t \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    102\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_score(y, y_pred)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\celisMeta\\FalseDiscovery.py:18\u001b[0m, in \u001b[0;36mFalseDiscovery.getValueForX\u001b[1;34m(self, dist, a, b, params, z_prior, x, return_probs)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetValueForX\u001b[39m(\u001b[38;5;28mself\u001b[39m, dist, a, b, params, z_prior, x, return_probs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 18\u001b[0m     u_1, u_2, l_1, l_2 \u001b[38;5;241m=\u001b[39m params\n\u001b[0;32m     19\u001b[0m     z_0, z_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mz_prior, z_prior\n\u001b[0;32m     21\u001b[0m     pos \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(x))\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "##### MODELING: META-ALGORITHM\n",
    "\n",
    "# timer\n",
    "cv_start = time.time()\n",
    "\n",
    "# Assuming data is in the format 'dataN' where N is the dataset number\n",
    "dataset_number = data[4:]\n",
    "\n",
    "# Create the directory path (assuming directories data1, data2, ..., data50 already exist)/\n",
    "input_dir = os.path.join(data_path, 'prepared', 'data' + dataset_number)\n",
    "output_dir = os.path.join(res_path, 'inprocess1', 'intermediate', 'data' + dataset_number)\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(num_folds):\n",
    "    \n",
    "    ##### LOAD DATA\n",
    "\n",
    "    # feedback\n",
    "    print('-'*30)\n",
    "    print('- FOLD ' + str(fold) + '...')\n",
    "    print('-'*30)\n",
    "    \n",
    "    # import data subsets\n",
    "    train_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_train.pkl')\n",
    "    valid_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_valid.pkl')\n",
    "    test_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_test.pkl')\n",
    "        \n",
    "    # Load the dataset\n",
    "    with open(train_path, 'rb') as file:\n",
    "        data_train = pickle.load(file)\n",
    "    with open(valid_path, 'rb') as file:\n",
    "        data_valid = pickle.load(file)\n",
    "    with open(test_path, 'rb') as file:\n",
    "        data_test = pickle.load(file)\n",
    "\n",
    "    ##### MODELING\n",
    "\n",
    "    # placeholders\n",
    "    meta_predictions_test  = pd.DataFrame()\n",
    "    meta_predictions_valid = pd.DataFrame()\n",
    "\n",
    "    # loop through tau\n",
    "    for tau in all_tau:\n",
    "        \n",
    "        # feedback\n",
    "        print('--- tau: %.2f' % tau)\n",
    "        colname = 'tau_' + str(tau)\n",
    "\n",
    "        # fit meta algorithm\n",
    "        debiased_model = MetaFairClassifier(tau = tau, sensitive_attr = protected)\n",
    "        try:\n",
    "            debiased_model.fit(data_train)\n",
    "        except ZeroDivisionError:\n",
    "            print('---- Error, using previous tau')\n",
    "            debiased_model = last_dm\n",
    "\n",
    "        # predict test scores\n",
    "        dataset_debiasing_test = debiased_model.predict(data_test)\n",
    "        scores_test            = dataset_debiasing_test.scores\n",
    "        meta_predictions_test[colname] = sum(scores_test.tolist(), [])\n",
    "        \n",
    "        # predict validation scores\n",
    "        dataset_debiasing_valid = debiased_model.predict(data_valid)\n",
    "        scores_valid            = dataset_debiasing_valid.scores\n",
    "        meta_predictions_valid[colname] = sum(scores_valid.tolist(), [])\n",
    "        \n",
    "        # save model\n",
    "        last_dm = debiased_model\n",
    "\n",
    "    # export CSV\n",
    "    meta_predictions_test.to_csv(os.path.join(output_dir, data + '_' +  str(fold) + '_MA_predictions_test.csv'),  index = None, header=True)\n",
    "    meta_predictions_valid.to_csv(os.path.join(output_dir, data + '_' + str(fold) + '_MA_predictions_valid.csv'), index = None, header=True)\n",
    "    print('')\n",
    "\n",
    "# print performance\n",
    "print('')\n",
    "print('Finished in {:.2f} minutes'.format((time.time() - cv_start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
