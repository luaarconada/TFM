{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAIR IN-PROCESSING\n",
    "\n",
    "This notebook implements the Adversarial Debiasing in-processor [(Zhang et al. 2018)](https://dl.acm.org/doi/abs/10.1145/3278721.3278779).\n",
    "\n",
    "The modeling is performed separately for each combination of training folds. This is controlled with `use_fold` variable. To fit adversarial debiasing on a different combination of training folds, set `use_fold` to a specific value and restar the kernel.\n",
    "\n",
    "A further analysis of the processor outputs is performed in `code_05_inprocess3.R`.\n",
    "\n",
    "The notebook loads the data exported in `code_00_partitinoing.ipynb` and applies in-processors. The processor predictions are exported as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "source": [
    "##### PACKAGES\n",
    "\n",
    "# working paths\n",
    "%run code_00_working_paths.py\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(func_path)\n",
    "\n",
    "from load_data import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameters and preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PARAMETERS\n",
    "\n",
    "# specify data set\n",
    "# one of ['data1', 'data2', ..., 'data50']\n",
    "data = 'data50'\n",
    "\n",
    "# partitioning\n",
    "num_folds = 5\n",
    "use_fold  = 2 # one of [0, 1, ..., 4 (num_folds-1)]\n",
    "seed      = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### IN-PROCESSOR PARAMS\n",
    "\n",
    "adversary_loss_weight = 0.1 # other options: [0.1, 0.01, 0.001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RANDOM SEED\n",
    "\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 6)\n"
     ]
    }
   ],
   "source": [
    "##### LOAD PARTITIONING\n",
    "\n",
    "# Assuming data is in the format 'dataN' where N is the dataset number\n",
    "dataset_number = data[4:]\n",
    "\n",
    "# Create the directory path (assuming directories data1, data2, ..., data50 already exist)\n",
    "input_dir = os.path.join(data_path, 'prepared', 'data' + dataset_number)\n",
    "\n",
    "# Construct the full file path\n",
    "file_path = os.path.join(input_dir, data + '_orig_test.pkl')\n",
    "\n",
    "# Load the dataset\n",
    "with open(file_path, 'rb') as file:\n",
    "    dataset_orig_test = pickle.load(file)\n",
    "    \n",
    "# Convert to dataframe and print the shape\n",
    "te = dataset_orig_test.convert_to_dataframe()[0]\n",
    "print(te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DATA PREP\n",
    "\n",
    "# protected attribute\n",
    "protected           = 'race'\n",
    "privileged_groups   = [{'race': 1}] \n",
    "unprivileged_groups = [{'race': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fair processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_4516\\3867462334.py:3: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_4516\\3867462334.py:3: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_4516\\3867462334.py:6: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_4516\\3867462334.py:6: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "- FOLD 2...\n",
      "------------------------------\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:164: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:164: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.675506; batch adversarial loss: 0.901261\n",
      "epoch 1; iter: 0; batch classifier loss: 0.675336; batch adversarial loss: 0.880142\n",
      "epoch 2; iter: 0; batch classifier loss: 0.661023; batch adversarial loss: 0.880418\n",
      "epoch 3; iter: 0; batch classifier loss: 0.686842; batch adversarial loss: 0.901248\n",
      "epoch 4; iter: 0; batch classifier loss: 0.687953; batch adversarial loss: 0.895749\n",
      "epoch 5; iter: 0; batch classifier loss: 0.632687; batch adversarial loss: 0.890215\n",
      "epoch 6; iter: 0; batch classifier loss: 0.640715; batch adversarial loss: 0.891794\n",
      "epoch 7; iter: 0; batch classifier loss: 0.669918; batch adversarial loss: 0.920905\n",
      "epoch 8; iter: 0; batch classifier loss: 0.691289; batch adversarial loss: 0.915743\n",
      "epoch 9; iter: 0; batch classifier loss: 0.658753; batch adversarial loss: 0.906337\n",
      "epoch 10; iter: 0; batch classifier loss: 0.685555; batch adversarial loss: 0.895702\n",
      "epoch 11; iter: 0; batch classifier loss: 0.734201; batch adversarial loss: 0.918991\n",
      "epoch 12; iter: 0; batch classifier loss: 0.738863; batch adversarial loss: 0.917859\n",
      "epoch 13; iter: 0; batch classifier loss: 0.744891; batch adversarial loss: 0.920359\n",
      "epoch 14; iter: 0; batch classifier loss: 0.717849; batch adversarial loss: 0.903674\n",
      "epoch 15; iter: 0; batch classifier loss: 0.744200; batch adversarial loss: 0.910046\n",
      "epoch 16; iter: 0; batch classifier loss: 0.773539; batch adversarial loss: 0.919648\n",
      "epoch 17; iter: 0; batch classifier loss: 0.778188; batch adversarial loss: 0.935104\n",
      "epoch 18; iter: 0; batch classifier loss: 0.803740; batch adversarial loss: 0.933184\n",
      "epoch 19; iter: 0; batch classifier loss: 0.852719; batch adversarial loss: 0.936328\n",
      "epoch 20; iter: 0; batch classifier loss: 0.768557; batch adversarial loss: 0.898498\n",
      "epoch 21; iter: 0; batch classifier loss: 0.851879; batch adversarial loss: 0.936823\n",
      "epoch 22; iter: 0; batch classifier loss: 0.795178; batch adversarial loss: 0.913750\n",
      "epoch 23; iter: 0; batch classifier loss: 0.899250; batch adversarial loss: 0.921321\n",
      "epoch 24; iter: 0; batch classifier loss: 0.785982; batch adversarial loss: 0.908345\n",
      "epoch 25; iter: 0; batch classifier loss: 0.799794; batch adversarial loss: 0.902360\n",
      "epoch 26; iter: 0; batch classifier loss: 0.896830; batch adversarial loss: 0.912620\n",
      "epoch 27; iter: 0; batch classifier loss: 0.954494; batch adversarial loss: 0.902147\n",
      "epoch 28; iter: 0; batch classifier loss: 0.940129; batch adversarial loss: 0.900506\n",
      "epoch 29; iter: 0; batch classifier loss: 0.912785; batch adversarial loss: 0.895055\n",
      "epoch 30; iter: 0; batch classifier loss: 0.955079; batch adversarial loss: 0.889958\n",
      "epoch 31; iter: 0; batch classifier loss: 1.029057; batch adversarial loss: 0.897569\n",
      "epoch 32; iter: 0; batch classifier loss: 0.973718; batch adversarial loss: 0.881400\n",
      "epoch 33; iter: 0; batch classifier loss: 0.993332; batch adversarial loss: 0.884273\n",
      "epoch 34; iter: 0; batch classifier loss: 1.020472; batch adversarial loss: 0.886798\n",
      "epoch 35; iter: 0; batch classifier loss: 1.157049; batch adversarial loss: 0.897283\n",
      "epoch 36; iter: 0; batch classifier loss: 1.060970; batch adversarial loss: 0.878712\n",
      "epoch 37; iter: 0; batch classifier loss: 1.105522; batch adversarial loss: 0.868593\n",
      "epoch 38; iter: 0; batch classifier loss: 0.995443; batch adversarial loss: 0.855690\n",
      "epoch 39; iter: 0; batch classifier loss: 1.023868; batch adversarial loss: 0.851786\n",
      "epoch 40; iter: 0; batch classifier loss: 1.038112; batch adversarial loss: 0.855358\n",
      "epoch 41; iter: 0; batch classifier loss: 1.045867; batch adversarial loss: 0.834943\n",
      "epoch 42; iter: 0; batch classifier loss: 1.032747; batch adversarial loss: 0.845255\n",
      "epoch 43; iter: 0; batch classifier loss: 1.050025; batch adversarial loss: 0.844133\n",
      "epoch 44; iter: 0; batch classifier loss: 1.119375; batch adversarial loss: 0.844326\n",
      "epoch 45; iter: 0; batch classifier loss: 1.117159; batch adversarial loss: 0.833020\n",
      "epoch 46; iter: 0; batch classifier loss: 1.120061; batch adversarial loss: 0.836660\n",
      "epoch 47; iter: 0; batch classifier loss: 1.155575; batch adversarial loss: 0.826640\n",
      "epoch 48; iter: 0; batch classifier loss: 1.105643; batch adversarial loss: 0.820624\n",
      "epoch 49; iter: 0; batch classifier loss: 1.077381; batch adversarial loss: 0.821697\n",
      "\n",
      "Finished in 0.06 minutes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Disable TensorFlow eager execution\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Create a TensorFlow session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Define the normalization function\n",
    "def safe_normalize(x):\n",
    "    norm = tf.norm(x)\n",
    "    print(\"x:\", x)\n",
    "    print(\"norm:\", norm)\n",
    "    return tf.cond(tf.not_equal(norm, 0), lambda: x / norm, lambda: x)\n",
    "\n",
    "##### MODELING\n",
    "\n",
    "# timer\n",
    "cv_start = time.time()\n",
    "\n",
    "# Create the directory path (assuming directories data1, data2, ..., data50 already exist)\n",
    "input_dir = os.path.join(data_path, 'prepared', 'data' + dataset_number)\n",
    "output_dir = os.path.join(res_path, 'inprocess2', 'intermediate', 'data' + dataset_number)\n",
    "\n",
    "# loop through training folds\n",
    "for fold in range(num_folds):\n",
    "    \n",
    "    ##### LOAD DATA\n",
    "    \n",
    "    # select fold combination\n",
    "    if fold != use_fold:\n",
    "        continue\n",
    "\n",
    "    # feedback\n",
    "    print('-'*30)\n",
    "    print('- FOLD ' + str(fold) + '...')\n",
    "    print('-'*30)\n",
    "\n",
    "    # import data subsets\n",
    "    train_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_train.pkl')\n",
    "    valid_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_valid.pkl')\n",
    "    test_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_test.pkl')\n",
    "        \n",
    "    # Load the dataset\n",
    "    with open(train_path, 'rb') as file:\n",
    "        data_train = pickle.load(file)\n",
    "    with open(valid_path, 'rb') as file:\n",
    "        data_valid = pickle.load(file)\n",
    "    with open(test_path, 'rb') as file:\n",
    "        data_test = pickle.load(file)\n",
    "\n",
    "    ##### MODELING\n",
    "\n",
    "    # fit adversarial debiasing\n",
    "    with tf.variable_scope('debiased_classifier', reuse=tf.AUTO_REUSE):\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups     = privileged_groups,\n",
    "                                              unprivileged_groups   = unprivileged_groups,\n",
    "                                              debias                = True,\n",
    "                                              adversary_loss_weight = adversary_loss_weight,\n",
    "                                              scope_name            = 'debiased_classifier',\n",
    "                                              sess                  = sess)\n",
    "        debiased_model.fit(data_train)\n",
    "    \n",
    "    # apply the model to valid data\n",
    "    scores_valid = debiased_model.predict(data_valid).scores\n",
    "    # Flatten scores_valid to 1D\n",
    "    scores_valid_flat = scores_valid.flatten()\n",
    "    advdebias_predictions = pd.DataFrame()\n",
    "    advdebias_predictions['scores'] = scores_valid_flat\n",
    "    advdebias_predictions['targets'] = data_valid.labels.flatten()\n",
    "    advdebias_predictions.to_csv(os.path.join(output_dir, data + '_' + str(fold) + '_AD_' + str(adversary_loss_weight) + str(use_fold) + '_predictions_valid.csv'), \n",
    "                                 index=None, \n",
    "                                 header=True)\n",
    "\n",
    "    # Apply the model to test data\n",
    "    scores_test = debiased_model.predict(data_test).scores\n",
    "    scores_test_flat = scores_test.flatten()\n",
    "\n",
    "    advdebias_predictions = pd.DataFrame()\n",
    "    advdebias_predictions['scores'] = scores_test_flat\n",
    "    advdebias_predictions.to_csv(os.path.join(output_dir, data + '_' + str(fold) + '_AD_' + str(adversary_loss_weight) + str(use_fold) + '_predictions_test.csv'), \n",
    "                                 index=None, \n",
    "                                 header=True)\n",
    "\n",
    "    # print performance\n",
    "    print('')\n",
    "    print('Finished in {:.2f} minutes'.format((time.time() - cv_start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
