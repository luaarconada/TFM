{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAIR IN-PROCESSING\n",
    "\n",
    "This notebook implements the Adversarial Debiasing in-processor [(Zhang et al. 2018)](https://dl.acm.org/doi/abs/10.1145/3278721.3278779).\n",
    "\n",
    "The modeling is performed separately for each combination of training folds. This is controlled with `use_fold` variable. To fit adversarial debiasing on a different combination of training folds, set `use_fold` to a specific value and restar the kernel.\n",
    "\n",
    "A further analysis of the processor outputs is performed in `code_05_inprocess3.R`.\n",
    "\n",
    "The notebook loads the data exported in `code_00_partitinoing.ipynb` and applies in-processors. The processor predictions are exported as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "source": [
    "##### PACKAGES\n",
    "\n",
    "# working paths\n",
    "%run code_00_working_paths.py\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(func_path)\n",
    "\n",
    "from load_data import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameters and preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PARAMETERS\n",
    "\n",
    "# specify data set\n",
    "# one of ['data1', 'data2', ..., 'data50']\n",
    "data = 'data50'\n",
    "\n",
    "# partitioning\n",
    "num_folds = 5\n",
    "use_fold  = 1 # one of [0, 1, ..., 4 (num_folds-1)]\n",
    "seed      = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### IN-PROCESSOR PARAMS\n",
    "\n",
    "adversary_loss_weight = 0.1 # other options: [0.1, 0.01, 0.001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RANDOM SEED\n",
    "\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 6)\n"
     ]
    }
   ],
   "source": [
    "##### LOAD PARTITIONING\n",
    "\n",
    "# Assuming data is in the format 'dataN' where N is the dataset number\n",
    "dataset_number = data[4:]\n",
    "\n",
    "# Create the directory path (assuming directories data1, data2, ..., data50 already exist)\n",
    "input_dir = os.path.join(data_path, 'prepared', 'data' + dataset_number)\n",
    "\n",
    "# Construct the full file path\n",
    "file_path = os.path.join(input_dir, data + '_orig_test.pkl')\n",
    "\n",
    "# Load the dataset\n",
    "with open(file_path, 'rb') as file:\n",
    "    dataset_orig_test = pickle.load(file)\n",
    "    \n",
    "# Convert to dataframe and print the shape\n",
    "te = dataset_orig_test.convert_to_dataframe()[0]\n",
    "print(te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DATA PREP\n",
    "\n",
    "# protected attribute\n",
    "protected           = 'race'\n",
    "privileged_groups   = [{'race': 1}] \n",
    "unprivileged_groups = [{'race': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fair processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_5468\\3867462334.py:3: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_5468\\3867462334.py:3: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_5468\\3867462334.py:6: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_5468\\3867462334.py:6: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "- FOLD 1...\n",
      "------------------------------\n",
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:164: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\aif360\\algorithms\\inprocessing\\adversarial_debiasing.py:164: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.675980; batch adversarial loss: 0.897796\n",
      "epoch 1; iter: 0; batch classifier loss: 0.672981; batch adversarial loss: 0.873351\n",
      "epoch 2; iter: 0; batch classifier loss: 0.660969; batch adversarial loss: 0.868586\n",
      "epoch 3; iter: 0; batch classifier loss: 0.683734; batch adversarial loss: 0.906971\n",
      "epoch 4; iter: 0; batch classifier loss: 0.673118; batch adversarial loss: 0.889020\n",
      "epoch 5; iter: 0; batch classifier loss: 0.629010; batch adversarial loss: 0.891740\n",
      "epoch 6; iter: 0; batch classifier loss: 0.611049; batch adversarial loss: 0.882281\n",
      "epoch 7; iter: 0; batch classifier loss: 0.632459; batch adversarial loss: 0.902446\n",
      "epoch 8; iter: 0; batch classifier loss: 0.664805; batch adversarial loss: 0.906682\n",
      "epoch 9; iter: 0; batch classifier loss: 0.594022; batch adversarial loss: 0.886805\n",
      "epoch 10; iter: 0; batch classifier loss: 0.673891; batch adversarial loss: 0.891137\n",
      "epoch 11; iter: 0; batch classifier loss: 0.672972; batch adversarial loss: 0.914377\n",
      "epoch 12; iter: 0; batch classifier loss: 0.688613; batch adversarial loss: 0.907724\n",
      "epoch 13; iter: 0; batch classifier loss: 0.722555; batch adversarial loss: 0.911145\n",
      "epoch 14; iter: 0; batch classifier loss: 0.677564; batch adversarial loss: 0.898598\n",
      "epoch 15; iter: 0; batch classifier loss: 0.743252; batch adversarial loss: 0.893707\n",
      "epoch 16; iter: 0; batch classifier loss: 0.763747; batch adversarial loss: 0.914737\n",
      "epoch 17; iter: 0; batch classifier loss: 0.754532; batch adversarial loss: 0.933969\n",
      "epoch 18; iter: 0; batch classifier loss: 0.765222; batch adversarial loss: 0.919615\n",
      "epoch 19; iter: 0; batch classifier loss: 0.764125; batch adversarial loss: 0.918694\n",
      "epoch 20; iter: 0; batch classifier loss: 0.709508; batch adversarial loss: 0.885866\n",
      "epoch 21; iter: 0; batch classifier loss: 0.829569; batch adversarial loss: 0.925745\n",
      "epoch 22; iter: 0; batch classifier loss: 0.785882; batch adversarial loss: 0.916641\n",
      "epoch 23; iter: 0; batch classifier loss: 0.786958; batch adversarial loss: 0.901612\n",
      "epoch 24; iter: 0; batch classifier loss: 0.773343; batch adversarial loss: 0.906559\n",
      "epoch 25; iter: 0; batch classifier loss: 0.764275; batch adversarial loss: 0.908488\n",
      "epoch 26; iter: 0; batch classifier loss: 0.761349; batch adversarial loss: 0.882544\n",
      "epoch 27; iter: 0; batch classifier loss: 0.860354; batch adversarial loss: 0.897291\n",
      "epoch 28; iter: 0; batch classifier loss: 0.887603; batch adversarial loss: 0.892934\n",
      "epoch 29; iter: 0; batch classifier loss: 0.837794; batch adversarial loss: 0.889265\n",
      "epoch 30; iter: 0; batch classifier loss: 0.842649; batch adversarial loss: 0.893227\n",
      "epoch 31; iter: 0; batch classifier loss: 0.997704; batch adversarial loss: 0.886843\n",
      "epoch 32; iter: 0; batch classifier loss: 0.927872; batch adversarial loss: 0.870784\n",
      "epoch 33; iter: 0; batch classifier loss: 0.921442; batch adversarial loss: 0.880615\n",
      "epoch 34; iter: 0; batch classifier loss: 0.968928; batch adversarial loss: 0.880709\n",
      "epoch 35; iter: 0; batch classifier loss: 1.096218; batch adversarial loss: 0.881900\n",
      "epoch 36; iter: 0; batch classifier loss: 1.011219; batch adversarial loss: 0.867889\n",
      "epoch 37; iter: 0; batch classifier loss: 1.041715; batch adversarial loss: 0.855486\n",
      "epoch 38; iter: 0; batch classifier loss: 0.956790; batch adversarial loss: 0.847650\n",
      "epoch 39; iter: 0; batch classifier loss: 1.029624; batch adversarial loss: 0.840804\n",
      "epoch 40; iter: 0; batch classifier loss: 1.012700; batch adversarial loss: 0.849178\n",
      "epoch 41; iter: 0; batch classifier loss: 0.974023; batch adversarial loss: 0.834973\n",
      "epoch 42; iter: 0; batch classifier loss: 1.012786; batch adversarial loss: 0.838728\n",
      "epoch 43; iter: 0; batch classifier loss: 0.983257; batch adversarial loss: 0.838722\n",
      "epoch 44; iter: 0; batch classifier loss: 1.153899; batch adversarial loss: 0.837426\n",
      "epoch 45; iter: 0; batch classifier loss: 1.028397; batch adversarial loss: 0.821294\n",
      "epoch 46; iter: 0; batch classifier loss: 1.079911; batch adversarial loss: 0.829046\n",
      "epoch 47; iter: 0; batch classifier loss: 1.156322; batch adversarial loss: 0.827018\n",
      "epoch 48; iter: 0; batch classifier loss: 1.050092; batch adversarial loss: 0.813646\n",
      "epoch 49; iter: 0; batch classifier loss: 0.994677; batch adversarial loss: 0.810853\n",
      "\n",
      "Finished in 0.06 minutes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Disable TensorFlow eager execution\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Create a TensorFlow session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Define the normalization function\n",
    "def safe_normalize(x):\n",
    "    norm = tf.norm(x)\n",
    "    print(\"x:\", x)\n",
    "    print(\"norm:\", norm)\n",
    "    return tf.cond(tf.not_equal(norm, 0), lambda: x / norm, lambda: x)\n",
    "\n",
    "##### MODELING\n",
    "\n",
    "# timer\n",
    "cv_start = time.time()\n",
    "\n",
    "# Create the directory path (assuming directories data1, data2, ..., data50 already exist)\n",
    "input_dir = os.path.join(data_path, 'prepared', 'data' + dataset_number)\n",
    "output_dir = os.path.join(res_path, 'inprocess2', 'intermediate', 'data' + dataset_number)\n",
    "\n",
    "# loop through training folds\n",
    "for fold in range(num_folds):\n",
    "    \n",
    "    ##### LOAD DATA\n",
    "    \n",
    "    # select fold combination\n",
    "    if fold != use_fold:\n",
    "        continue\n",
    "\n",
    "    # feedback\n",
    "    print('-'*30)\n",
    "    print('- FOLD ' + str(fold) + '...')\n",
    "    print('-'*30)\n",
    "\n",
    "    # import data subsets\n",
    "    train_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_train.pkl')\n",
    "    valid_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_valid.pkl')\n",
    "    test_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_test.pkl')\n",
    "        \n",
    "    # Load the dataset\n",
    "    with open(train_path, 'rb') as file:\n",
    "        data_train = pickle.load(file)\n",
    "    with open(valid_path, 'rb') as file:\n",
    "        data_valid = pickle.load(file)\n",
    "    with open(test_path, 'rb') as file:\n",
    "        data_test = pickle.load(file)\n",
    "\n",
    "    ##### MODELING\n",
    "\n",
    "    # fit adversarial debiasing\n",
    "    with tf.variable_scope('debiased_classifier', reuse=tf.AUTO_REUSE):\n",
    "        debiased_model = AdversarialDebiasing(privileged_groups     = privileged_groups,\n",
    "                                              unprivileged_groups   = unprivileged_groups,\n",
    "                                              debias                = True,\n",
    "                                              adversary_loss_weight = adversary_loss_weight,\n",
    "                                              scope_name            = 'debiased_classifier',\n",
    "                                              sess                  = sess)\n",
    "        debiased_model.fit(data_train)\n",
    "    \n",
    "    # apply the model to valid data\n",
    "    scores_valid = debiased_model.predict(data_valid).scores\n",
    "    # Flatten scores_valid to 1D\n",
    "    scores_valid_flat = scores_valid.flatten()\n",
    "    advdebias_predictions = pd.DataFrame()\n",
    "    advdebias_predictions['scores'] = scores_valid_flat\n",
    "    advdebias_predictions['targets'] = data_valid.labels.flatten()\n",
    "    advdebias_predictions.to_csv(os.path.join(output_dir, data + '_' + str(fold) + '_AD_' + str(adversary_loss_weight) + str(use_fold) + '_predictions_valid.csv'), \n",
    "                                 index=None, \n",
    "                                 header=True)\n",
    "\n",
    "    # Apply the model to test data\n",
    "    scores_test = debiased_model.predict(data_test).scores\n",
    "    scores_test_flat = scores_test.flatten()\n",
    "\n",
    "    advdebias_predictions = pd.DataFrame()\n",
    "    advdebias_predictions['scores'] = scores_test_flat\n",
    "    advdebias_predictions.to_csv(os.path.join(output_dir, data + '_' + str(fold) + '_AD_' + str(adversary_loss_weight) + str(use_fold) + '_predictions_test.csv'), \n",
    "                                 index=None, \n",
    "                                 header=True)\n",
    "\n",
    "    # print performance\n",
    "    print('')\n",
    "    print('Finished in {:.2f} minutes'.format((time.time() - cv_start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
