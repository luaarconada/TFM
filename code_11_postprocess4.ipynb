{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAIR POST-PROCESSING\n",
    "\n",
    "This notebook implements the Reject Option Classification post-processor [(Kamiran et al. 2012)](https://ieeexplore.ieee.org/abstract/document/6413831).\n",
    "\n",
    "The notebook applies reject option classification post-processir to classifier predictions. It loads the data exported in `code_00_partitioning.ipynb` and predictions of base classifers produced in `code_08_postprocess1.R`. The post-processed predictions are exported as CSV files. A further analysis of the processor outputs is performed in `code_12_postprocess5.R`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    }
   ],
   "source": [
    "##### PACKAGES\n",
    "\n",
    "# working paths\n",
    "%run code_00_working_paths.py\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.postprocessing.reject_option_classification import RejectOptionClassification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(func_path)\n",
    "\n",
    "from load_data import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameters and preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PARAMETERS\n",
    "\n",
    "# specify data set\n",
    "# one of ['data1', ..., 'data50']\n",
    "data = 'data1'\n",
    "\n",
    "# partitioning\n",
    "num_folds = 10\n",
    "seed      = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### POST-PROCESSOR PARAMS\n",
    "\n",
    "metric_name      = 'Statistical parity difference'\n",
    "num_class_thresh = 100\n",
    "num_ROC_margin   = 50\n",
    "all_bound        = [0.1, 0.2, 0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RANDOM SEED\n",
    "\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 6)\n"
     ]
    }
   ],
   "source": [
    "##### LOAD PARTITIONING\n",
    "\n",
    "# Assuming data is in the format 'dataN' where N is the dataset number\n",
    "dataset_number = data[4:]\n",
    "\n",
    "# Create the directory path (assuming directories data1, data2, ..., data50 already exist)\n",
    "input_dir = os.path.join(data_path, 'prepared')\n",
    "\n",
    "# Construct the full file path\n",
    "file_path = os.path.join(input_dir, data + '_orig_test.pkl')\n",
    "\n",
    "# Load the dataset\n",
    "with open(file_path, 'rb') as file:\n",
    "    dataset_orig_test = pickle.load(file)\n",
    "    \n",
    "# Convert to dataframe and print the shape\n",
    "te = dataset_orig_test.convert_to_dataframe()[0]\n",
    "print(te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DATA PREP\n",
    "# protected attribute\n",
    "protected           = 'race'\n",
    "privileged_groups   = [{'race': 1}] \n",
    "unprivileged_groups = [{'race': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fair processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "- FOLD 0...\n",
      "------------------------------\n",
      "-- BOUND 0.1...\n",
      "-- BOUND 0.2...\n",
      "-- BOUND 0.3...\n",
      "\n",
      "------------------------------\n",
      "- FOLD 1...\n",
      "------------------------------\n",
      "-- BOUND 0.1...\n",
      "-- BOUND 0.2...\n",
      "-- BOUND 0.3...\n",
      "\n",
      "------------------------------\n",
      "- FOLD 2...\n",
      "------------------------------\n",
      "-- BOUND 0.1...\n",
      "-- BOUND 0.2...\n",
      "-- BOUND 0.3...\n",
      "\n",
      "------------------------------\n",
      "- FOLD 3...\n",
      "------------------------------\n",
      "-- BOUND 0.1...\n",
      "-- BOUND 0.2...\n",
      "-- BOUND 0.3...\n",
      "\n",
      "------------------------------\n",
      "- FOLD 4...\n",
      "------------------------------\n",
      "-- BOUND 0.1...\n",
      "-- BOUND 0.2...\n",
      "-- BOUND 0.3...\n",
      "\n",
      "------------------------------\n",
      "- FOLD 5...\n",
      "------------------------------\n",
      "-- BOUND 0.1...\n",
      "-- BOUND 0.2...\n",
      "-- BOUND 0.3...\n",
      "\n",
      "------------------------------\n",
      "- FOLD 6...\n",
      "------------------------------\n",
      "-- BOUND 0.1...\n",
      "-- BOUND 0.2...\n",
      "-- BOUND 0.3...\n",
      "\n",
      "------------------------------\n",
      "- FOLD 7...\n",
      "------------------------------\n",
      "-- BOUND 0.1...\n",
      "-- BOUND 0.2...\n",
      "-- BOUND 0.3...\n",
      "\n",
      "------------------------------\n",
      "- FOLD 8...\n",
      "------------------------------\n",
      "-- BOUND 0.1...\n",
      "-- BOUND 0.2...\n",
      "-- BOUND 0.3...\n",
      "\n",
      "------------------------------\n",
      "- FOLD 9...\n",
      "------------------------------\n",
      "-- BOUND 0.1...\n",
      "-- BOUND 0.2...\n",
      "-- BOUND 0.3...\n",
      "\n",
      "\n",
      "Finished in 8.40 minutes\n"
     ]
    }
   ],
   "source": [
    "##### MODELING\n",
    "\n",
    "# timer\n",
    "cv_start = time.time()\n",
    "\n",
    "# Create the directory path (assuming directories data1, data2, ..., data50 already exist)\n",
    "input_dir = os.path.join(data_path, 'prepared')\n",
    "output_dir = os.path.join(res_path, 'pre1post4', 'intermediate')\n",
    "\n",
    "# base models\n",
    "model_names = ['glm', \n",
    "                \"rf\", \n",
    "                #\"xgbTree\", \n",
    "                \"nnet\"]\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(num_folds):\n",
    "    \n",
    "    ##### LOAD DATA\n",
    "    \n",
    "    # feedback\n",
    "    print('-'*30)\n",
    "    print('- FOLD ' + str(fold) + '...')\n",
    "    print('-'*30)\n",
    "\n",
    "    # import data subsets\n",
    "    train_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_train.pkl')\n",
    "    valid_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_valid.pkl')\n",
    "    test_path = os.path.join(input_dir, data + '_scaled_' + str(fold) + '_test.pkl')\n",
    "        \n",
    "    # Load the dataset\n",
    "    with open(train_path, 'rb') as file:\n",
    "        data_train = pickle.load(file)\n",
    "    with open(valid_path, 'rb') as file:\n",
    "        data_valid = pickle.load(file)\n",
    "    with open(test_path, 'rb') as file:\n",
    "        data_test = pickle.load(file)\n",
    "\n",
    "\n",
    "    ##### MODELING\n",
    "    \n",
    "    # import prediction results from R\n",
    "    dataset_trainResults_valid = pd.read_csv(res_path + 'postprocess1/' + 'intermediate/' + data + '_' + str(fold) + '_POST_training_results_dval.csv')\n",
    "    dataset_trainResults_test  = pd.read_csv(res_path + 'postprocess1/' + 'intermediate/' + data + '_' + str(fold) + '_POST_training_results_dtest.csv')\n",
    "    \n",
    "    # copy preds\n",
    "    dataset_orig_valid_pred = data_valid.copy(deepcopy = True)\n",
    "    dataset_orig_test_pred  = data_test.copy(deepcopy  = True)\n",
    "    \n",
    "    \n",
    "    # loop through bound values\n",
    "    for i in all_bound:\n",
    "        \n",
    "        # feedback\n",
    "        print('-- BOUND ' + str(i) + '...')\n",
    "    \n",
    "        # placeholder\n",
    "        ROC_test = pd.DataFrame()\n",
    "\n",
    "        # loop through base classifiers\n",
    "        for m in model_names:\n",
    "\n",
    "            # extract validation preds\n",
    "            scores_valid = np.array(dataset_trainResults_valid[m + '_scores']).reshape(len(dataset_trainResults_valid.index),1)\n",
    "            labels_valid = np.where(dataset_trainResults_valid[m + '_class'] == 'Parole', 1.0, 2.0).reshape(len(dataset_trainResults_valid.index), 1)\n",
    "\n",
    "            # extract test preds\n",
    "            scores_test = np.array(dataset_trainResults_test[m + '_scores']).reshape(len(dataset_trainResults_test.index),1)\n",
    "            labels_test = np.where(dataset_trainResults_test[m + '_class'] == 'Parole', 1.0, 2.0).reshape(len(dataset_trainResults_test.index), 1)\n",
    "\n",
    "            # write predictions\n",
    "            dataset_orig_valid_pred.scores = scores_valid\n",
    "            dataset_orig_valid_pred.labels = labels_valid\n",
    "            dataset_orig_test_pred.scores  = scores_test\n",
    "            dataset_orig_test_pred.labels  = labels_test\n",
    "\n",
    "            # fit ROC\n",
    "            ROC = RejectOptionClassification(unprivileged_groups = unprivileged_groups, \n",
    "                                                privileged_groups   = privileged_groups, \n",
    "                                                num_class_thresh    = num_class_thresh, \n",
    "                                                num_ROC_margin      = num_ROC_margin,\n",
    "                                                metric_name         = metric_name,\n",
    "                                                metric_ub           = i, \n",
    "                                                metric_lb           = -i)\n",
    "            ROC = ROC.fit(data_valid, dataset_orig_valid_pred)\n",
    "\n",
    "            # predict test scores\n",
    "            dataset_transf_test_pred    = ROC.predict(dataset_orig_test_pred)\n",
    "            ROC_test[m + \"_fairScores\"] = dataset_transf_test_pred.scores.flatten()\n",
    "            label_names                 = np.where(dataset_transf_test_pred.labels == 1, 'Parole', 'Noparole')\n",
    "            ROC_test[m + \"_fairLabels\"] = label_names\n",
    "\n",
    "        # export CSV\n",
    "        ROC_test.to_csv(os.path.join(output_dir, data + '_' + str(fold) + '_ROC_' + str(i) + '_predictions_test.csv'),  index = None, header=True)\n",
    "         \n",
    "    # feedback\n",
    "    print('')\n",
    "\n",
    "# print performance\n",
    "print('')\n",
    "print('Finished in {:.2f} minutes'.format((time.time() - cv_start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
